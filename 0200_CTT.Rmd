# R Code for Module 5

```{r include = FALSE}
#install.packages("devtools")
#devtools::install_github("talbano/epmr")
source("common.R")
library(tweetrmd) #... embedding tweets
library(tidyverse)
library(psych)

```

```{r links, child="links.md"}
```

## Simulate a constant true score

In this example, I simulate a constant true score, and randomly varying error scores from a normal population with mean 0 and SD 1
Note, set.seed() gives R a starting point for generating random numbers, so we can get the same results on different computers

You should check the mean and SD of E and X. Creating a histogram of X might be interesting too...

```{r}

set.seed(160416)
myt <- 20
mye <- rnorm(1000, mean = 0, sd = 1)
myx <- myt + mye

df=data.frame(myx,myt,mye)
mean(myx); sd(myx)

p <- ggplot(df, aes(x=myx)) + 
  geom_histogram()
p

```

## PISA total reading scores with simulated error and true scores based on CTT

```{r}

## Libraries

#install.packages("devtools")
#devtools::install_github("talbano/epmr")
library(epmr) 
library(ggplot2)


ritems <- c("r414q02", "r414q11", "r414q06", "r414q09", 
  "r452q03", "r452q04", "r452q06", "r452q07", "r458q01",
  "r458q07", "r458q04")
rsitems <- paste0(ritems, "s")
xscores <- rowSums(PISA09[PISA09$cnt == "BEL", rsitems],
  na.rm = TRUE)
```

Simulate error scores based on known SEM of 1.4, which we'll calculate later, then create true scores
True scores are truncated to fall between 0 and 11 using setrange()

```{r}
escores <- rnorm(length(xscores), 0, 1.4)
tscores <- setrange(xscores - escores, y = xscores)

```

Combine in a data frame and create a scatterplot

```{r}

scores <- data.frame(x1 = xscores, t = tscores,
  e = escores)
ggplot(scores, aes(x1, t)) +
  geom_point(position = position_jitter(w = .3)) +
  geom_abline(col = "blue")
```


## Reliability and unreliability Illustrated


Here we have simulated scores for a new form of the reading test called y. Note that rho is the made up reliability, which is set to 0.80, and x is the original reading total scores. Form y, which is slightly easier than x, has a mean of 6 and SD of 3.

```{r cttplot2, fig.cap = "PISA total reading scores and scores on a simulated second form of the reading test."}

xysim <- rsim(rho = .8, x = scores$x1, meany = 6, sdy = 3)
scores$y <- round(setrange(xysim$y, scores$x1))
ggplot(scores, aes(x1, y)) +
  geom_point(position = position_jitter(w = .3, h = .3)) +
  geom_abline(col = "blue")
```



# (PART) Module 06 {-}


# R Code and Readings for Module 6


## Statistical definition of reliability. 

In CTT, reliability is defined as the proportion of variability in $X$ that is due to variability in true scores $T$:

\begin{equation}
r = \frac{\sigma^2_T}{\sigma^2_X}.
(\#eq:rel1)
\end{equation}


### Estimating reliability

One indirect estimate made possible by CTT is the correlation between scores on two forms of the same test:

\begin{equation}
r = \rho_{X_1 X_2} = \frac{\sigma_{X_1 X_2}}{\sigma_{X_1} \sigma_{X_2}}.
(\#eq:rel2)
\end{equation}

### Split-half method

The split-half method takes scores on a single test form, and separates them into scores on two halves of the test, which are treated as separate test forms. The correlation between these two halves then represents an indirect estimate of reliability, based on Equation \@ref(eq:rel1).

```{r}
# Split half correlation, assuming we only had scores on 
# one test form
# With an odd number of reading items, one half has 5 
# items and the other has 6
xsplit1 <- rowSums(PISA09[PISA09$cnt == "BEL", 
  rsitems[1:5]])
xsplit2 <- rowSums(PISA09[PISA09$cnt == "BEL", 
  rsitems[6:11]])
cor(xsplit1, xsplit2, use = "complete")
```

### Spearman Brown

The Spearman-Brown formula was originally used to correct for the reduction in reliability that occurred when correlating two test forms that were only half the length of the original test. In theory, reliability will increase as we add items to a test. Thus, Spearman-Brown is used to estimate, or predict, what the reliability would be if the half-length tests were made into full-length tests.

```{r}
# sb_r() in the epmr package uses the Spearman-Brown 
# formula to estimate how reliability would change when 
# test length changes by a factor k
# If test length were doubled, k would be 2
sb_r(r = cor(xsplit1, xsplit2, use = "complete"), k = 2)
```

The Spearman-Brown reliability, $r_{new}$, is estimated as a function of what's labeled here as the old reliability, $r_{old}$, and the factor by which the length of $X$ is predicted to change, $k$:

\begin{equation}
r_{new} = \frac{kr_{old}}{(k - 1)r_{old} + 1}.
(\#eq:rnew)
\end{equation}


Again, $k$ is the factor by which the test length is increased or decreased. It is equal to the number of items in the new test divided by the number of items in the original test. Multiply $k$ by the old reliability, and then divided the result by $(k - 1)$ times the old reliability, plus 1. The epmr package contains `sb_r()`, a simple function for estimating the Spearman-Brown reliability.


## SEM

Typically, we're more interested in how the unreliability of a test can be expressed in terms of the available observed variability. Thus, we multiply the unreliable proportion of variance by the standard deviation of $X$ to obtain the SEM:

\begin{equation}
SEM = \sigma_X\sqrt{1 - r}.
(\#eq:sem)
\end{equation}



Confidence intervals for `PISA09` can be estimated in the same way. First, we choose a measure of reliability, find the SD of observed scores, and obtain the corresponding SEM. Then, we can find the CI, which gives us the expected amount of uncertainty in our observed scores due to random measurement error. Here, we're calculating SEM and the CI using alpha, but other reliability estimates would work as well. Figure \@ref(fig:cttplot3) shows the 11 possible `PISA09` reading scores in order, with error bars based on SEM for students in Belgium.

```{r cttplot3, out.width='75%', fig.cap = "The PISA09 reading scale shown with 68 and 95 percent confidence intervals around each point."}
# Get alpha and SEM for students in Belgium

bela <- coef_alpha(PISA09[PISA09$cnt == "BEL", rsitems])
bela_alpha=bela$alpha
# The sem function from epmr also overlaps with sem from 
# another package so we're spelling it out here in long 
# form
belsem <- epmr::sem(r = bela_alpha, sd = sd(scores$x1,
  na.rm = TRUE))
# Plot the 11 possible total scores against themselves
# Error bars are shown for 1 SEM, giving a 68% confidence
# interval and 2 SEM, giving the 95% confidence interval
# x is converted to factor to show discrete values on the 
# x-axis
beldat <- data.frame(x = 1:11, sem = belsem)
ggplot(beldat, aes(factor(x), x)) +
  geom_errorbar(aes(ymin = x - sem * 2,
    ymax = x + sem * 2), col = "violet") +
  geom_errorbar(aes(ymin = x - sem, ymax = x + sem),
    col = "yellow") +
  geom_point()
```
Figure \@ref(fig:cttplot3) helps us visualize the impact of unreliable measurement on score comparisons. For example, note that the top of the 95% confidence interval for $X$ of 2 extends nearly to 5 points, and thus overlaps with the CI for adjacent scores 3 through 7. It isn't until $X$ of 8 that the CI no longer overlap. With a CI of `belsem` `r round(belsem, 3)`, we're 95% confident that students with observed scores differing at least by `belsem * 4` `r round(belsem, 3) * 4` have different true scores. Students with observed scores closer than `belsem * 4` may actually have the same true scores.



### Interpreting reliability and unreliability

There are no agreed-upon standards for interpreting reliability coefficients. Reliability is bound by 0 on the lower end and 1 at the upper end, because, by definition, the amount of true variability can never be less or more than the total available variability in $X$. Higher reliability is clearly better, but cutoffs for acceptable levels of reliability vary for different fields, situations, and types of tests. The stakes of a test are an important consideration when interpreting reliability coefficients. The higher the stakes, the higher we expect reliability to be. Otherwise, cutoffs depend on the particular application.

In general, reliabilities for educational and psychological tests can be interpreted using scales like the ones presented in Table \@ref(tab:interpretr). With medium-stakes tests, a reliability of 0.70 is sometimes considered minimally acceptable, 0.80 is decent, 0.90 is quite good, and anything above 0.90 is excellent. High stakes tests should have reliabilities at or above 0.90. Low stakes tests, which are often simpler and shorter than higher-stakes ones, often have reliabilities as low as 0.70. These are general guidelines, and interpretations can vary considerably by test. Remember that the cognitive measures in PISA would be considered low-stakes at the student level.

A few additional considerations are necessary when interpreting coefficient alpha. First, alpha assumes that all items measure the same single construct. Items are also assumed to be equally related to this construct, that is, they are assumed to be parallel measures of the construct. When the items are not parallel measures of the construct, alpha is considered a lower-bound estimate of reliability, that is, the true reliability for the test is expected to be higher than indicated by alpha. Finally, alpha is not a measure of dimensionality. It is frequently claimed that a strong coefficient alpha supports the unidimensionality of a measure. However, alpha does not index dimensionality. It is impacted by the extent to which all of the test items measure a single construct, but it does not necessarily go up or down as a test becomes more or less unidimensional.

```{r interpretr, echo=FALSE}
if (!knitr:::is_latex_output()) {
knitr::kable(cbind("Reliability" = c("$\\geq 0.90$", "$0.80 \\leq r < 0.90$", "$0.70 \\leq r < 0.80$", "$0.60 \\leq r < 0.70$", "$0.50 \\leq r < 0.60$", "$0.20 \\leq r < 0.50$", "$0.00 \\leq r < 0.20$"), "High Stakes Interpretation" = c("Excellent", "Good", "Acceptable", "Borderline", "Low", "Unacceptable", "Unacceptable"), "Low Stakes Interpretation" = c("Excellent", "Excellent", "Good", "Acceptable", "Borderline", "Low", "Unacceptable")),
  digits = 2, caption = "General Guidelines for Interpreting Reliability Coefficients")
} else {
knitr::kable(cbind("Reliability" = c("$>= 0.90$", "$0.80 <= r < 0.90$", "$0.70 <= r < 0.80$", "$0.60 <= r < 0.70$", "$0.50 <= r < 0.60$", "$0.20 <= r < 0.50$", "$0.00 <= r < 0.20$"), "High Stakes Interpretation" = c("Excellent", "Good", "Acceptable", "Borderline", "Low", "Unacceptable", "Unacceptable"), "Low Stakes Interpretation" = c("Excellent", "Excellent", "Good", "Acceptable", "Borderline", "Low", "Unacceptable")),
  digits = 2, caption = "General Guidelines for Interpreting Reliability Coefficients")
}

```







