[["index.html", "Psychological Testing Course Notes Welcome to PSY 362", " Psychological Testing Course Notes S. Mason Garrison 2021-02-27 Welcome to PSY 362 Welcome to class! This website is designed to accompany Mason Garrisons Psychological Testing (Psych Testing). Psych Testing is a undergraduate-level quantitative methods course at Wake Forest University. This website hosts the course notes. Im glad you found me and the notes! They only cover the R aspects of this class. Why are the notes over here? And not on canvas? Well, that is a good question! Frankly, because this is much much easier for me to create. Moreover, if Im teaching you about R, you might as well learn it using R. What does that mean? Well, these notes are written in R (and RMarkdown)! Indeed, this entire website is written in R! You can find the current version of the course syllabus here, along with all of the other syllabi for my classes. "],["attribution.html", "Attribution Major &amp; Additional Attributions", " Attribution This class leans heavily on other peoples materials and ideas. I have done my best to document the origin of the materials and ideas. In particular, I have noted those people whose work has been a major contribution as well as those who have additional contributions. You can see specific changes by examining the edit history on the git repo Major &amp; Additional Attributions Jenny Bryans (jennybryan.org) STAT 545. Anthony Albanos Course Notes on Introduction to Educational and Psychological Measurement Bill Revelles An introduction to psychometric theory with applications in R "],["license.html", "License", " License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by-sa/4.0/legalcode for the full legal text. You are free to: Sharecopy and redistribute the material in any medium or format Remixremix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: AttributionYou must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlikeIf you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictionsYou may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. "],["colophon.html", "Colophon", " Colophon This book was written in bookdown inside RStudio. The website is hosted with github, The complete source is available from GitHub. The book style was designed by Desir√©e De Leon. This version of the book was built with: #&gt; Finding R package dependencies ... Done! #&gt; setting value #&gt; version R version 4.0.3 (2020-10-10) #&gt; os Windows 10 x64 #&gt; system x86_64, mingw32 #&gt; ui RTerm #&gt; language (EN) #&gt; collate English_United States.1252 #&gt; ctype English_United States.1252 #&gt; tz America/New_York #&gt; date 2021-02-27 Along with these packages: "],["public-health-dashboards.html", "Public Health Dashboards Wake Forest Forsyth County North Carolina", " Public Health Dashboards Ok, so I know that this class is about testing, measurement, and R However, I figured that it might be helpful for us all to have some public health dashboards in one easy place. If you want, we can pretend that this section is course content related Wake Forest The embedded dashboards are maintained by Wake Forest University and are made using Microsoft Power BI. More info about the dashboard can be found here here Forsyth County The embedded maps are maintained by Forsyth Countys department of public health. More info here Vaccinations Case Counts North Carolina "],["meet-our-toolbox.html", "1 Meet our toolbox! 1.1 R 1.2 Getting Help with R", " 1 Meet our toolbox! You can follow along with the slides here if they do not appear below. 1.1 R R is both a programming language and software environment for statistical analysis. It differs from other software like SPSS in three key ways. First, R is free, no strings (or warranties) attached. Download it at cran.r-project.org. The popular editor RStudio is also available for free at rstudio.com. Second, R is open-source, with thousands of active contributors sharing add-on packages. See the full list at cran.r-project.org/web/packages (there are currently over 8,000 packages). Third, R is accessed primarily through code, rather than by pointing and clicking through drop-down menus and dialog windows. This third point is a road block to some, but it ends up being a strength in the long run. Note well be using two pieces of software. R is the software that runs all your analyses. RStudio is an Integrated Development Environment or IDE that simplifies your interaction with R. RStudio isnt essential, but it gives you nice features for saving your R code, organizing the output of your analyses, and managing your add-on packages. 1.1.1 Install R and RStudio At this point you should download and install R and RStudio using the links below. The internet abounds with helpful tips on installation and getting started. I made a video walking windows-folks through the process. library(vembedr) embed_url(&quot;https://www.youtube.com/watch?v=kVIZGCT5p9U&quot;) %&gt;% use_align(&quot;center&quot;) Install R, a free software environment for statistical computing and graphics from CRAN, the Comprehensive R Archive Network. I highly recommend you install a precompiled binary distribution for your operating system  use the links up at the top of the CRAN page linked above! Install RStudios IDE (stands for integrated development environment), a powerful user interface for R. Get the Open Source Edition of RStudio Desktop. You can run either the Preview version or the official releases available here. RStudio comes with a text editor, so there is no immediate need to install a separate stand-alone editor. If you have a pre-existing installation of R and/or RStudio, I highly recommend that you reinstall both and get as current as possible. It can be considerably harder to run old software than new. If you upgrade R, you will need to update any packages you have installed. The command below should get you started, though you may need to specify more arguments if, e.g., you have been using a non-default library for your packages. update.packages(ask = FALSE, checkBuilt = TRUE) Note: this code will only look for updates on CRAN. So if you use a package that lives only on GitHub or if you want a development version from GitHub, you will need to update manually, e.g. via devtools::install_github(). 1.1.2 Testing testing Do whatever is appropriate for your OS to launch RStudio. You should get a window similar to the screenshot you see here, but yours will be more boring because you havent written any code or made any figures yet! Put your cursor in the pane labeled Console, which is where you interact with the live R process. Create a simple object with code like x &lt;- 3 * 4 (followed by enter or return). Then inspect the x object by typing x followed by enter or return. You should see the value 12 print to screen. If yes, youve succeeded in installing R and RStudio. As noted above, R is accessed via code, primarily in the form of commands that youll type or paste in the R console. The R console is simply the window where your R code goes, and where output will appear. Note that RStudio will present you with multiple windows, one of which will be the R console. That said, when instructions here say to run code in R, this applies to R via RStudio as well. 1.1.3 Add-on packages R is an extensible system and many people share useful code they have developed as a package via CRAN and GitHub. To install a package from CRAN, for example the dplyr package for data manipulation, here is one way to do it in the R console (there are others). install.packages(&quot;dplyr&quot;, dependencies = TRUE) By including dependencies = TRUE, we are being explicit and extra-careful to install any additional packages the target package, dplyr in the example above, needs to have around. You could use the above method to install the following packages, all of which we will use: tidyr, package webpage ggplot2, package webpage 1.2 Getting Help with R You can follow along with the slides here if they do not appear below. Help files in R are easily accessed by entering a function name preceded by a question mark, for example, ?c, ?mean, or ?devtools::install_github. Parentheses arent necessary. The question mark approach is a shortcut for the help() function, where, for example, ?mean is the same as help(\"mean\"). Either approach provides direct access to the R documentation for a function. The documentation for a function should at least give you a brief description of the function, definitions for the arguments that the function accepts, and examples of how the function can be used. At the console, you can also perform a search through all of the available R documentation using two question marks. For example, ??\"regression\" will search the R documentation for the term regression. This is a shortcut for the function help.search(). Finally, you can browse through the R documentation with help.start(). This will open a web browser with links to manuals and other resources. If youre unsatisfied with the documentation internal to R, an online search can be surprisingly effective for finding function names or instructions on running a certain procedure or analysis in R. 1.2.1 Further resources The above will get your basic setup ready but here are some links if you are interested in reading a bit further. How to Use RStudio RStudios leads for learning R R FAQ R Installation and Administration More about add-on packages in the R Installation and Administration Manual "],["r-basics.html", "2 R basics and workflows 2.1 Introducing Functions 2.2 Introduction to Objects 2.3 Workspace and working directory 2.4 RStudio projects 2.5 Saving Files and Protips", " 2 R basics and workflows Who is R? Why is R troubling PhD students?@AcademicChatter #AcademicTwitter&mdash; Dr. Marie Curie (@CurieDr) January 31, 2021 Well start our tour of R with a summary of how R code is used to interact with R via the console. In these notes, blocks of example R code are offset from the main text as shown below. Comments within code blocks start with a single hash #, the code itself has nothing consistent preceding it, and output from my R console is preceded by a double hash ##. You can copy and paste example code directly into your R console. Anything after the # will be ignored. # This is a comment within a block of R code. Comments # start with the hash sign and are ignored by R. The code # below will be interpreted by R once you paste or type it # into the console. x &lt;- c(4, 8, 15, 16, 23, 42) mean(x) # Only code after the hash is ignored #&gt; [1] 18 sd(x) #&gt; [1] 13.5 In the code above, were creating a short vector of scores in x and calculating its mean and standard deviation. Ok, if you havent opened up R/RStudio, do that now. Notice the default panes: Console (entire left) Environment/History (tabbed in upper right) Files/Plots/Packages/Help (tabbed in lower right) FYI: you can change the default location of the panes, among many other things: Customizing RStudio. Go into the Console, where we interact with the live R process. You should paste that example code in the console and verify that you get the same results. Note that code you enter at the console is preceded by the R prompt &gt;, whereas output printed in your console is not. 2.1 Introducing Functions x &lt;- c(4, 8, 15, 16, 23, 42) mean(x) #&gt; [1] 18 sd(x) #&gt; [1] 13.5 In the example code above (and in the previous section), the functions that get things done in R have names. To use them, we summon by name with parentheses enclosing any required information or instructions for how the functions should work. For example, we used the function c() to combine a set of numbers into a vector of scores. The information supplied to c() consisted of the scores themselves, separated by commas. mean() and sd() are functions for obtaining the mean and standard deviation of vectors of scores, like the ones in x. 2.2 Introduction to Objects x &lt;- c(4, 8, 15, 16, 23, 42) mean(x) #&gt; [1] 18 sd(x) #&gt; [1] 13.5 The second thing to notice in the example above is that data and results are saved to objects in R using the assignment operator &lt;-. We used the concatenate function to stick our numbers together in a set, c(4, 8, 15, 16, 23, 42), and then we assigned the result to have the name x. Objects created in this way can be accessed later on by their assigned name, for example, to find a mean or standard deviation. If we wanted to access it later, we could also save the mean of x to a new object. 2.2.0.1 Now you try! Make an assignment and then inspect the object you just created: x &lt;- 3 * 4 x #&gt; [1] 12 All R statements where you create objects  assignments  have this form: objectName &lt;- value and in my head I hear, e.g., x gets 12. Object names cannot start with a digit and cannot contain certain other characters such as a comma or a space. You will be wise to adopt a convention for demarcating words in names. i_use_snake_case other.people.use.periods evenOthersUseCamelCase Make another assignment: this_is_a_really_long_name &lt;- 2.5 To inspect this, try out RStudios completion facility: type the first few characters, press TAB, add characters until you disambiguate, then press return. Make another assignment: mason_rocks &lt;- 2 ^ 3 Lets try to inspect: masonrocks #&gt; Error in eval(expr, envir, enclos): object &#39;masonrocks&#39; not found masn_rocks #&gt; Error in eval(expr, envir, enclos): object &#39;masn_rocks&#39; not found Implicit contract with the computer / scripting language: Computer will do tedious computation for you. In return, you will be completely precise in your instructions. Typos matter. Case matters. mason_rocks #&gt; [1] 8 R has a mind-blowing collection of built-in functions that are accessed like so: functionName(arg1 = val1, arg2 = val2, and so on) Lets try using seq() which makes regular sequences of numbers and, while were at it, demo more helpful features of RStudio. Type se and hit TAB. A pop up shows you possible completions. Specify seq() by typing more to disambiguate or using the up/down arrows to select. Notice the floating tool-tip-type help that pops up, reminding you of a functions arguments. If you want even more help, press F1 as directed to get the full documentation in the help tab of the lower right pane. Now open the parentheses and notice the automatic addition of the closing parenthesis and the placement of cursor in the middle. Type the arguments 1, 10 and hit return. RStudio also exits the parenthetical expression for you. IDEs are great. seq(1, 10) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 The above also demonstrates something about how R resolves function arguments. You can always specify in name = value form. But if you do not, R attempts to resolve by position. So above, it is assumed that we want a sequence from = 1 that goes to = 10. Since we didnt specify step size, the default value of by in the function definition is used, which ends up being 1 in this case. For functions I call often, I might use this resolve by position for the first argument or maybe the first two. After that, I always use name = value. Make this assignment and notice similar help with quotation marks. yo &lt;- &quot;hello world&quot; If you just make an assignment, you dont get to see the value, so then youre tempted to immediately inspect. y &lt;- seq(1, 10) y #&gt; [1] 1 2 3 4 5 6 7 8 9 10 This common action can be shortened by surrounding the assignment with parentheses, which causes assignment and print to screen to happen. (y &lt;- seq(1, 10)) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 Not all functions have (or require) arguments: date() #&gt; [1] &quot;Sat Feb 27 12:38:08 2021&quot; Now look at your workspace  in the upper right pane. The workspace is where user-defined objects accumulate. You can also get a listing of these objects with commands: objects() #&gt; [1] &quot;check_quietly&quot; &quot;ds4p_urls&quot; #&gt; [3] &quot;install_quietly&quot; &quot;mason_rocks&quot; #&gt; [5] &quot;pretty_install&quot; &quot;sample_no_surprises&quot; #&gt; [7] &quot;shhh_check&quot; &quot;slide_url&quot; #&gt; [9] &quot;this_is_a_really_long_name&quot; &quot;x&quot; #&gt; [11] &quot;y&quot; &quot;yo&quot; ls() #&gt; [1] &quot;check_quietly&quot; &quot;ds4p_urls&quot; #&gt; [3] &quot;install_quietly&quot; &quot;mason_rocks&quot; #&gt; [5] &quot;pretty_install&quot; &quot;sample_no_surprises&quot; #&gt; [7] &quot;shhh_check&quot; &quot;slide_url&quot; #&gt; [9] &quot;this_is_a_really_long_name&quot; &quot;x&quot; #&gt; [11] &quot;y&quot; &quot;yo&quot; If you want to remove the object named y, you can do this: rm(y) To remove everything: rm(list = ls()) or click the broom in RStudios Environment pane. 2.3 Workspace and working directory One day you will need to quit R, go do something else and return to your analysis later. One day you will have multiple analyses going that use R and you want to keep them separate. One day you will need to bring data from the outside world into R and send numerical results and figures from R back out into the world. To handle these real life situations, you need to make two decisions: What about your analysis is real, i.e. will you save it as your lasting record of what happened? Where does your analysis live? 2.3.1 Workspace, .RData As a beginning R user, its OK to consider your workspace real. Very soon, I urge you to evolve to the next level, where you consider your saved R scripts as real. (In either case, of course the input data is very much real and requires preservation!) With the input data and the R code you used, you can reproduce everything. You can make your analysis fancier. You can get to the bottom of puzzling results and discover and fix bugs in your code. You can reuse the code to conduct similar analyses in new projects. You can remake a figure with different aspect ratio or save is as TIFF instead of PDF. You are ready to take questions. You are ready for the future. If you regard your workspace as real (saving and reloading all the time), if you need to redo analysis  youre going to either redo a lot of typing (making mistakes all the way) or will have to mine your R history for the commands you used. Rather than becoming an expert on managing the R history, a better use of your time and energy is to keep your good R code in a script for future reuse. Because it can be useful sometimes, note the commands youve recently run appear in the History pane. But you dont have to choose right now and the two strategies are not incompatible. Lets demo the save / reload the workspace approach. Upon quitting R, you have to decide if you want to save your workspace, for potential restoration the next time you launch R. Depending on your set up, R or your IDE, e.g. RStudio, will probably prompt you to make this decision. Quit R/RStudio, either from the menu, using a keyboard shortcut, or by typing q() in the Console. Youll get a prompt like this: Save workspace image to ~/.Rdata? Note where the workspace image is to be saved and then click Save. Using your favorite method, visit the directory where image was saved and verify there is a file named .RData. You will also see a file .Rhistory, holding the commands submitted in your recent session. Restart RStudio. In the Console you will see a line like this: [Workspace loaded from ~/.RData] indicating that your workspace has been restored. Look in the Workspace pane and youll see the same objects as before. In the History tab of the same pane, you should also see your command history. Youre back in business. This way of starting and stopping analytical work will not serve you well for long but its a start. 2.3.2 Working directory Any process running on your computer has a notion of its working directory. In R, this is where R will look, by default, for files you ask it to load. It also where, by default, any files you write to disk will go. Chances are your current working directory is the directory we inspected above, i.e. the one where RStudio wanted to save the workspace. You can explicitly check your working directory with: getwd() It is also displayed at the top of the RStudio console. As a beginning R user, its OK let your home directory or any other weird directory on your computer be Rs working directory. Very soon, I urge you to evolve to the next level, where you organize your projects into directories and, when working on project A, set Rs working directory to the associated directory. Although I do not recommend it, in case youre curious, you can set Rs working directory at the command line like so: setwd(&quot;~/myCoolProject&quot;) Although I do not recommend it, you can also use RStudios Files pane to navigate to a directory and then set it as working directory from the menu: Session &gt; Set Working Directory &gt; To Files Pane Location. (Youll see even more options there). Or within the Files pane, choose More and Set As Working Directory. But theres a better way. A way that also puts you on the path to managing your R work like an expert. 2.4 RStudio projects Keeping all the files associated with a project organized together  input data, R scripts, results, figures  is such a wise and common practice that RStudio has built-in support for this via its projects. Lets make one to use for the rest of this class. Do this: File &gt; New Project. The directory name you choose here will be the project name. Call it whatever you want (or follow me for convenience). I created a directory and, therefore RStudio project, called swc in my tmp directory, FYI. setwd(&quot;~/tmp/swc&quot;) Now check that the home directory for your project is the working directory of our current R process: getwd() I cant print my output here because this document itself does not reside in the RStudio Project we just created. Lets enter a few commands in the Console, as if we are just beginning a project: a &lt;- 2 b &lt;- -3 sig_sq &lt;- 0.5 x &lt;- runif(40) y &lt;- a + b * x + rnorm(40, sd = sqrt(sig_sq)) (avg_x &lt;- mean(x)) #&gt; [1] 0.455 write(avg_x, &quot;avg_x.txt&quot;) plot(x, y) abline(a, b, col = &quot;purple&quot;) dev.print(pdf, &quot;toy_line_plot.pdf&quot;) #&gt; png #&gt; 2 Lets say this is a good start of an analysis and your ready to start preserving the logic and code. Visit the History tab of the upper right pane. Select these commands. Click To Source. Now you have a new pane containing a nascent R script. Click on the floppy disk to save. Give it a name ending in .R or .r, I used toy-line.r and note that, by default, it will go in the directory associated with your project. Quit RStudio. Inspect the folder associated with your project if you wish. Maybe view the PDF in an external viewer. Restart RStudio. Notice that things, by default, restore to where we were earlier, e.g. objects in the workspace, the command history, which files are open for editing, where we are in the file system browser, the working directory for the R process, etc. These are all Good Things. Change some things about your code. Top priority would be to set a sample size n at the top, e.g. n &lt;- 40, and then replace all the hard-wired 40s with n. Change some other minor-but-detectable stuff, e.g. alter the sample size n, the slope of the line b,the color of the line  whatever. Practice the different ways to re-run the code: Walk through line by line by keyboard shortcut (Command+Enter) or mouse (click Run in the upper right corner of editor pane). Source the entire document  equivalent to entering source('toy-line.r') in the Console  by keyboard shortcut (Shift+Command+S) or mouse (click Source in the upper right corner of editor pane or select from the mini-menu accessible from the associated down triangle). Source with echo from the Source mini-menu. Visit your figure in an external viewer to verify that the PDF is changing as you expect. In your favorite OS-specific way, search your files for toy_line_plot.pdf and presumably you will find the PDF itself (no surprise) but also the script that created it (toy-line.r). This latter phenomenon is a huge win. One day you will want to remake a figure or just simply understand where it came from. If you rigorously save figures to file with R code and not ever ever ever the mouse or the clipboard, you will sing my praises one day. Trust me. 2.5 Saving Files and Protips It is traditional to save R scripts with a .R or .r suffix. Follow this convention unless you have some extraordinary reason not to. Comments start with one or more # symbols. Use them. RStudio helps you (de)comment selected lines with Ctrl+Shift+C (Windows and Linux) or Command+Shift+C (Mac). Clean out the workspace, i.e. pretend like youve just revisited this project after a long absence. The broom icon or rm(list = ls()). Good idea to do this, restart R (available from the Session menu), re-run your analysis to truly check that the code youre saving is complete and correct (or at least rule out obvious problems!). This workflow will serve you well in the future: Create an RStudio project for an analytical project Keep inputs there (well soon talk about importing) Keep scripts there; edit them, run them in bits or as a whole from there Keep outputs there (like the PDF written above) Avoid using the mouse for pieces of your analytical workflow, such as loading a dataset or saving a figure. Terribly important for reproducibility and for making it possible to retrospectively determine how a numerical table or PDF was actually produced (searching on local disk on filename, among .R files, will lead to the relevant script). Many long-time users never save the workspace, never save .RData files (Im one of them), never save or consult the history. Once/if you get to that point, there are options available in RStudio to disable the loading of .RData and permanently suppress the prompt on exit to save the workspace (go to Tools &gt; Options &gt; General). For the record, when loading data into R and/or writing outputs to file, you can always specify the absolute path and thereby insulate yourself from the current working directory. This method is rarely necessary when using RStudio projects. "],["refreshers-and-examples.html", "3 Refreshers and Examples 3.1 Some terms 3.2 Summary Statistics", " 3 Refreshers and Examples Youre now ready for a review of the introductory statistics that are prerequisite for the analyses that come later in this class. Statistics are important in measurement because they allow us to score and summarize the information collected with our tests and instruments. Theyre used to describe the reliability, validity, and predictive power of this information. Theyre also used to describe how well our test covers a domain of content or a network of constructs, including in relation to other content areas or constructs. We rely heavily on statistics for later modules 3.1 Some terms Well begin this review with some basic statistical terms. These ideas should be familiar to you from your research methods classes, but I figured that this information is useful in case youve gotten a little rusty. First, a variable is a set of values that can differ for different people. For example, we often measure variables such as age and gender. These words are italicized here to denote them as statistical variables, as opposed to words. The term variable is synonymous with quality, attribute, trait, or property. Constructs are also variables. Really, a variable is anything assigned to people that can potentially take on more than just a single constant value. As noted above, variables in R can be contained within simple vectors, for example, x, or they can be grouped together in a data.frame. Generic variables will be labeled in this book using capital letters, usually \\(X\\) and \\(Y\\). Here, \\(X\\) might represent a generic test score, for example, the total score across all the items in a test. It might also represent scores on a single item. Both are considered variables. The definition of a generic variable like \\(X\\) depends on the context in which it is defined. Indices can also be used to denote generic variables that are part of some sequence of variables. Most often this will be scores on items within a test, where, for example, \\(X_1\\) is the first item, \\(X_2\\) is the second, and \\(X_J\\) is the last, with \\(J\\) being the number of items in the test and \\(X_j\\) representing any given item. Subscripts can also be used to index individual people on a single variable. For example, test scores for a group of people could be denoted as \\(X_1\\), \\(X_2\\), \\(\\dots\\), \\(X_N\\), where \\(N\\) is the number of people and \\(X_i\\) represents the score for a generic person. Combining people and items, \\(X_{ij}\\) would be the score for person \\(i\\) on item \\(j\\). The number of people is denoted by \\(n\\) or sometimes \\(N\\). Typically, the lowercase \\(n\\) represents sample size and the uppercase \\(N\\) represents the population, however, the two are often used interchangeably. Greek and Arabic letters are used for other sample and population statistics. The sample mean is denoted by \\(m\\) and the population mean by \\(\\mu\\), the standard deviation is \\(s\\) or \\(\\sigma\\), variance is \\(s^2\\) or \\(\\sigma^2\\), and correlation is \\(r\\) or \\(\\rho\\). Note that the mean and standard deviation are sometimes abbreviated as \\(M\\) and \\(SD\\). Note also that distinctions between sample and population values often arent necessary, in which case the population terms are used. If a distinction is necessary, it will be identified. Finally, you may see named subscripts added to variable names and other terms, for example, \\(M_{control}\\) might denote the mean of a control group. These subscripts depend on the situation and must be interpreted in context. 3.2 Summary Statistics Descriptive and inferential are terms that refer to two general uses of statistics. These uses differ based on whether or not an inference is made from properties of a sample of data to parameters for an unknown population. Descriptive statistics, or descriptives, are used simply to explore and describe certain features of distributions. For example, the mean and variance are statistics identifying the center of and variability in a distribution. These and other statistics are used inferentially when an inference is made to a population Descriptives are not typically used to answer research questions or inform decision making. Instead, inferential statistics are more appropriate for these less exploratory and more confirmatory results. Inferential statistics involve an inference to a parameter or a population value. The quality of this inference is gauged using statistical tests that index the error associated with our estimates. In this review were focusing on descriptive statistics. Later well consider some inferential applications. The describe() function in the psych package returns basic descriptive statistics that are often useful for psychometrics, including the mean, median, standard deviation (sd), skewness (skew), kurtosis (kurt), minimum (min), and maximum (max), as well as some others Im forgetting. The describe function in the psych package is meant to produce the most frequently requested stats in psychometric and psychology studies, and to produce them in an easy to read data.frame. If a grouping variable is called for in formula mode, it will also call describeBy to the processing. The results from describe can be used in graphics functions (e.g., error.crosses). The range statistics (min, max, range) are most useful for data checking to detect coding errors, and should be found in early analyses of the data. Although describe will work on data frames as well as matrices, it is important to realize that for data frames, descriptive statistics will be reported only for those variables where this makes sense (i.e., not for alphanumeric data). If the check option is TRUE, variables that are categorical or logical are converted to numeric and then described. These variables are marked with an * in the row name. This is somewhat slower. Note that in the case of categories or factors, the numerical ordering is not necessarily the one expected. For instance, if education is coded high school, some college , finished college, then the default coding will lead to these as values of 2, 3, 1. Thus, statistics for those variables marked with * should be interpreted cautiously (if at all). "],["r-code-for-module-5.html", "4 R Code for Module 5 4.1 Simulate a constant true score 4.2 PISA total reading scores with simulated error and true scores based on CTT 4.3 Reliability and unreliability Illustrated", " 4 R Code for Module 5 4.1 Simulate a constant true score In this example, I simulate a constant true score, and randomly varying error scores from a normal population with mean 0 and SD 1 Note, set.seed() gives R a starting point for generating random numbers, so we can get the same results on different computers You should check the mean and SD of E and X. Creating a histogram of X might be interesting too set.seed(160416) myt &lt;- 20 mye &lt;- rnorm(1000, mean = 0, sd = 1) myx &lt;- myt + mye df=data.frame(myx,myt,mye) mean(myx); sd(myx) #&gt; [1] 20 #&gt; [1] 1.03 p &lt;- ggplot(df, aes(x=myx)) + geom_histogram() p #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 4.2 PISA total reading scores with simulated error and true scores based on CTT ## Libraries #install.packages(&quot;devtools&quot;) #devtools::install_github(&quot;talbano/epmr&quot;) library(epmr) #&gt; #&gt; Attaching package: &#39;epmr&#39; #&gt; The following object is masked from &#39;package:psych&#39;: #&gt; #&gt; skew library(ggplot2) ritems &lt;- c(&quot;r414q02&quot;, &quot;r414q11&quot;, &quot;r414q06&quot;, &quot;r414q09&quot;, &quot;r452q03&quot;, &quot;r452q04&quot;, &quot;r452q06&quot;, &quot;r452q07&quot;, &quot;r458q01&quot;, &quot;r458q07&quot;, &quot;r458q04&quot;) rsitems &lt;- paste0(ritems, &quot;s&quot;) xscores &lt;- rowSums(PISA09[PISA09$cnt == &quot;BEL&quot;, rsitems], na.rm = TRUE) Simulate error scores based on known SEM of 1.4, which well calculate later, then create true scores True scores are truncated to fall between 0 and 11 using setrange() escores &lt;- rnorm(length(xscores), 0, 1.4) tscores &lt;- setrange(xscores - escores, y = xscores) Combine in a data frame and create a scatterplot scores &lt;- data.frame(x1 = xscores, t = tscores, e = escores) ggplot(scores, aes(x1, t)) + geom_point(position = position_jitter(w = .3)) + geom_abline(col = &quot;blue&quot;) 4.3 Reliability and unreliability Illustrated Here we have simulated scores for a new form of the reading test called y. Note that rho is the made up reliability, which is set to 0.80, and x is the original reading total scores. Form y, which is slightly easier than x, has a mean of 6 and SD of 3. xysim &lt;- rsim(rho = .8, x = scores$x1, meany = 6, sdy = 3) scores$y &lt;- round(setrange(xysim$y, scores$x1)) ggplot(scores, aes(x1, y)) + geom_point(position = position_jitter(w = .3, h = .3)) + geom_abline(col = &quot;blue&quot;) Figure 4.1: PISA total reading scores and scores on a simulated second form of the reading test. "],["r-code-and-readings-for-module-6.html", "5 R Code and Readings for Module 6 5.1 Statistical definition of reliability. 5.2 SEM", " 5 R Code and Readings for Module 6 5.1 Statistical definition of reliability. In CTT, reliability is defined as the proportion of variability in \\(X\\) that is due to variability in true scores \\(T\\): \\[\\begin{equation} r = \\frac{\\sigma^2_T}{\\sigma^2_X}. \\tag{5.1} \\end{equation}\\] 5.1.1 Estimating reliability One indirect estimate made possible by CTT is the correlation between scores on two forms of the same test: \\[\\begin{equation} r = \\rho_{X_1 X_2} = \\frac{\\sigma_{X_1 X_2}}{\\sigma_{X_1} \\sigma_{X_2}}. \\tag{5.2} \\end{equation}\\] 5.1.2 Split-half method The split-half method takes scores on a single test form, and separates them into scores on two halves of the test, which are treated as separate test forms. The correlation between these two halves then represents an indirect estimate of reliability, based on Equation (5.1). # Split half correlation, assuming we only had scores on # one test form # With an odd number of reading items, one half has 5 # items and the other has 6 xsplit1 &lt;- rowSums(PISA09[PISA09$cnt == &quot;BEL&quot;, rsitems[1:5]]) xsplit2 &lt;- rowSums(PISA09[PISA09$cnt == &quot;BEL&quot;, rsitems[6:11]]) cor(xsplit1, xsplit2, use = &quot;complete&quot;) #&gt; [1] 0.625 5.1.3 Spearman Brown The Spearman-Brown formula was originally used to correct for the reduction in reliability that occurred when correlating two test forms that were only half the length of the original test. In theory, reliability will increase as we add items to a test. Thus, Spearman-Brown is used to estimate, or predict, what the reliability would be if the half-length tests were made into full-length tests. # sb_r() in the epmr package uses the Spearman-Brown # formula to estimate how reliability would change when # test length changes by a factor k # If test length were doubled, k would be 2 sb_r(r = cor(xsplit1, xsplit2, use = &quot;complete&quot;), k = 2) #&gt; [1] 0.769 The Spearman-Brown reliability, \\(r_{new}\\), is estimated as a function of whats labeled here as the old reliability, \\(r_{old}\\), and the factor by which the length of \\(X\\) is predicted to change, \\(k\\): \\[\\begin{equation} r_{new} = \\frac{kr_{old}}{(k - 1)r_{old} + 1}. \\tag{5.3} \\end{equation}\\] Again, \\(k\\) is the factor by which the test length is increased or decreased. It is equal to the number of items in the new test divided by the number of items in the original test. Multiply \\(k\\) by the old reliability, and then divided the result by \\((k - 1)\\) times the old reliability, plus 1. The epmr package contains sb_r(), a simple function for estimating the Spearman-Brown reliability. 5.2 SEM Typically, were more interested in how the unreliability of a test can be expressed in terms of the available observed variability. Thus, we multiply the unreliable proportion of variance by the standard deviation of \\(X\\) to obtain the SEM: \\[\\begin{equation} SEM = \\sigma_X\\sqrt{1 - r}. \\tag{5.4} \\end{equation}\\] Confidence intervals for PISA09 can be estimated in the same way. First, we choose a measure of reliability, find the SD of observed scores, and obtain the corresponding SEM. Then, we can find the CI, which gives us the expected amount of uncertainty in our observed scores due to random measurement error. Here, were calculating SEM and the CI using alpha, but other reliability estimates would work as well. Figure 5.1 shows the 11 possible PISA09 reading scores in order, with error bars based on SEM for students in Belgium. # Get alpha and SEM for students in Belgium bela &lt;- coef_alpha(PISA09[PISA09$cnt == &quot;BEL&quot;, rsitems]) bela_alpha=bela$alpha # The sem function from epmr also overlaps with sem from # another package so we&#39;re spelling it out here in long # form belsem &lt;- epmr::sem(r = bela_alpha, sd = sd(scores$x1, na.rm = TRUE)) # Plot the 11 possible total scores against themselves # Error bars are shown for 1 SEM, giving a 68% confidence # interval and 2 SEM, giving the 95% confidence interval # x is converted to factor to show discrete values on the # x-axis beldat &lt;- data.frame(x = 1:11, sem = belsem) ggplot(beldat, aes(factor(x), x)) + geom_errorbar(aes(ymin = x - sem * 2, ymax = x + sem * 2), col = &quot;violet&quot;) + geom_errorbar(aes(ymin = x - sem, ymax = x + sem), col = &quot;yellow&quot;) + geom_point() Figure 5.1: The PISA09 reading scale shown with 68 and 95 percent confidence intervals around each point. Figure 5.1 helps us visualize the impact of unreliable measurement on score comparisons. For example, note that the top of the 95% confidence interval for \\(X\\) of 2 extends nearly to 5 points, and thus overlaps with the CI for adjacent scores 3 through 7. It isnt until \\(X\\) of 8 that the CI no longer overlap. With a CI of belsem 1.425, were 95% confident that students with observed scores differing at least by belsem * 4 5.7 have different true scores. Students with observed scores closer than belsem * 4 may actually have the same true scores. 5.2.1 Interpreting reliability and unreliability There are no agreed-upon standards for interpreting reliability coefficients. Reliability is bound by 0 on the lower end and 1 at the upper end, because, by definition, the amount of true variability can never be less or more than the total available variability in \\(X\\). Higher reliability is clearly better, but cutoffs for acceptable levels of reliability vary for different fields, situations, and types of tests. The stakes of a test are an important consideration when interpreting reliability coefficients. The higher the stakes, the higher we expect reliability to be. Otherwise, cutoffs depend on the particular application. In general, reliabilities for educational and psychological tests can be interpreted using scales like the ones presented in Table 5.1. With medium-stakes tests, a reliability of 0.70 is sometimes considered minimally acceptable, 0.80 is decent, 0.90 is quite good, and anything above 0.90 is excellent. High stakes tests should have reliabilities at or above 0.90. Low stakes tests, which are often simpler and shorter than higher-stakes ones, often have reliabilities as low as 0.70. These are general guidelines, and interpretations can vary considerably by test. Remember that the cognitive measures in PISA would be considered low-stakes at the student level. A few additional considerations are necessary when interpreting coefficient alpha. First, alpha assumes that all items measure the same single construct. Items are also assumed to be equally related to this construct, that is, they are assumed to be parallel measures of the construct. When the items are not parallel measures of the construct, alpha is considered a lower-bound estimate of reliability, that is, the true reliability for the test is expected to be higher than indicated by alpha. Finally, alpha is not a measure of dimensionality. It is frequently claimed that a strong coefficient alpha supports the unidimensionality of a measure. However, alpha does not index dimensionality. It is impacted by the extent to which all of the test items measure a single construct, but it does not necessarily go up or down as a test becomes more or less unidimensional. Table 5.1: General Guidelines for Interpreting Reliability Coefficients Reliability High Stakes Interpretation Low Stakes Interpretation \\(\\geq 0.90\\) Excellent Excellent \\(0.80 \\leq r &lt; 0.90\\) Good Excellent \\(0.70 \\leq r &lt; 0.80\\) Acceptable Good \\(0.60 \\leq r &lt; 0.70\\) Borderline Acceptable \\(0.50 \\leq r &lt; 0.60\\) Low Borderline \\(0.20 \\leq r &lt; 0.50\\) Unacceptable Low \\(0.00 \\leq r &lt; 0.20\\) Unacceptable Unacceptable "],["good-resources.html", "6 Good Resources", " 6 Good Resources https://psychnerdjae.github.io/into-the-tidyverse/ Automatic Grading with RMarkdown example Git/Github for virtual learning (from this tweet) Learn-Datascience-for-Free https://allisonhorst.shinyapps.io/dplyr-learnr/ "],["references.html", "References", " References "]]
