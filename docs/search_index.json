[["index.html", "Psychological Testing Course Notes Welcome to PSY 362", " Psychological Testing Course Notes S. Mason Garrison 2021-03-06 Welcome to PSY 362 Welcome to class! This website is designed to accompany Mason Garrisons Psychological Testing (Psych Testing). Psych Testing is a undergraduate-level quantitative methods course at Wake Forest University. This website hosts the course notes. Im glad you found me and the notes! They only cover the R aspects of this class. Why are the notes over here? And not on canvas? Well, that is a good question! Frankly, because this is much much easier for me to create. Moreover, if Im teaching you about R, you might as well learn it using R. What does that mean? Well, these notes are written in R (and RMarkdown)! Indeed, this entire website is written in R! You can find the current version of the course syllabus here, along with all of the other syllabi for my classes. "],["attribution.html", "Attribution Major &amp; Additional Attributions", " Attribution This class leans heavily on other peoples materials and ideas. I have done my best to document the origin of the materials and ideas. In particular, I have noted those people whose work has been a major contribution as well as those who have additional contributions. You can see specific changes by examining the edit history on the git repo Major &amp; Additional Attributions Jenny Bryans (jennybryan.org) STAT 545. Anthony Albanos Course Notes on Introduction to Educational and Psychological Measurement Bill Revelles An introduction to psychometric theory with applications in R "],["license.html", "License", " License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by-sa/4.0/legalcode for the full legal text. You are free to: Sharecopy and redistribute the material in any medium or format Remixremix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: AttributionYou must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlikeIf you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictionsYou may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. "],["colophon.html", "Colophon", " Colophon This book was written in bookdown inside RStudio. The website is hosted with github, The complete source is available from GitHub. The book style was designed by Desir√©e De Leon. This version of the book was built with: #&gt; Finding R package dependencies ... Done! #&gt; setting value #&gt; version R version 4.0.3 (2020-10-10) #&gt; os Windows 10 x64 #&gt; system x86_64, mingw32 #&gt; ui RTerm #&gt; language (EN) #&gt; collate English_United States.1252 #&gt; ctype English_United States.1252 #&gt; tz America/New_York #&gt; date 2021-03-06 Along with these packages: "],["public-health-dashboards.html", "Public Health Dashboards Wake Forest Forsyth County North Carolina", " Public Health Dashboards Ok, so I know that this class is about testing, measurement, and R However, I figured that it might be helpful for us all to have some public health dashboards in one easy place. If you want, we can pretend that this section is course content related Wake Forest The embedded dashboards are maintained by Wake Forest University and are made using Microsoft Power BI. More info about the dashboard can be found here here Forsyth County The embedded maps are maintained by Forsyth Countys department of public health. More info here Vaccinations Case Counts North Carolina "],["meet-our-toolbox.html", "1 Meet our toolbox! 1.1 R 1.2 Getting Help with R", " 1 Meet our toolbox! You can follow along with the slides here if they do not appear below. 1.1 R R is both a programming language and software environment for statistical analysis. It differs from other software like SPSS in three key ways. First, R is free, no strings (or warranties) attached. Download it at cran.r-project.org. The popular editor RStudio is also available for free at rstudio.com. Second, R is open-source, with thousands of active contributors sharing add-on packages. See the full list at cran.r-project.org/web/packages (there are currently over 8,000 packages). Third, R is accessed primarily through code, rather than by pointing and clicking through drop-down menus and dialog windows. This third point is a road block to some, but it ends up being a strength in the long run. Note well be using two pieces of software. R is the software that runs all your analyses. RStudio is an Integrated Development Environment or IDE that simplifies your interaction with R. RStudio isnt essential, but it gives you nice features for saving your R code, organizing the output of your analyses, and managing your add-on packages. 1.1.1 Install R and RStudio At this point you should download and install R and RStudio using the links below. The internet abounds with helpful tips on installation and getting started. I made a video walking windows-folks through the process. library(vembedr) embed_url(&quot;https://www.youtube.com/watch?v=kVIZGCT5p9U&quot;) %&gt;% use_align(&quot;center&quot;) Install R, a free software environment for statistical computing and graphics from CRAN, the Comprehensive R Archive Network. I highly recommend you install a precompiled binary distribution for your operating system  use the links up at the top of the CRAN page linked above! Install RStudios IDE (stands for integrated development environment), a powerful user interface for R. Get the Open Source Edition of RStudio Desktop. You can run either the Preview version or the official releases available here. RStudio comes with a text editor, so there is no immediate need to install a separate stand-alone editor. If you have a pre-existing installation of R and/or RStudio, I highly recommend that you reinstall both and get as current as possible. It can be considerably harder to run old software than new. If you upgrade R, you will need to update any packages you have installed. The command below should get you started, though you may need to specify more arguments if, e.g., you have been using a non-default library for your packages. update.packages(ask = FALSE, checkBuilt = TRUE) Note: this code will only look for updates on CRAN. So if you use a package that lives only on GitHub or if you want a development version from GitHub, you will need to update manually, e.g. via devtools::install_github(). 1.1.2 Testing testing Do whatever is appropriate for your OS to launch RStudio. You should get a window similar to the screenshot you see here, but yours will be more boring because you havent written any code or made any figures yet! Put your cursor in the pane labeled Console, which is where you interact with the live R process. Create a simple object with code like x &lt;- 3 * 4 (followed by enter or return). Then inspect the x object by typing x followed by enter or return. You should see the value 12 print to screen. If yes, youve succeeded in installing R and RStudio. As noted above, R is accessed via code, primarily in the form of commands that youll type or paste in the R console. The R console is simply the window where your R code goes, and where output will appear. Note that RStudio will present you with multiple windows, one of which will be the R console. That said, when instructions here say to run code in R, this applies to R via RStudio as well. 1.1.3 Add-on packages R is an extensible system and many people share useful code they have developed as a package via CRAN and GitHub. To install a package from CRAN, for example the dplyr package for data manipulation, here is one way to do it in the R console (there are others). install.packages(&quot;dplyr&quot;, dependencies = TRUE) By including dependencies = TRUE, we are being explicit and extra-careful to install any additional packages the target package, dplyr in the example above, needs to have around. You could use the above method to install the following packages, all of which we will use: tidyr, package webpage ggplot2, package webpage psych, package webpage 1.2 Getting Help with R You can follow along with the slides here if they do not appear below. Help files in R are easily accessed by entering a function name preceded by a question mark, for example, ?c, ?mean, or ?devtools::install_github. Parentheses arent necessary. The question mark approach is a shortcut for the help() function, where, for example, ?mean is the same as help(\"mean\"). Either approach provides direct access to the R documentation for a function. The documentation for a function should at least give you a brief description of the function, definitions for the arguments that the function accepts, and examples of how the function can be used. At the console, you can also perform a search through all of the available R documentation using two question marks. For example, ??\"regression\" will search the R documentation for the term regression. This is a shortcut for the function help.search(). Finally, you can browse through the R documentation with help.start(). This will open a web browser with links to manuals and other resources. If youre unsatisfied with the documentation internal to R, an online search can be surprisingly effective for finding function names or instructions on running a certain procedure or analysis in R. 1.2.1 Further resources The above will get your basic setup ready but here are some links if you are interested in reading a bit further. How to Use RStudio RStudios leads for learning R R FAQ R Installation and Administration More about add-on packages in the R Installation and Administration Manual "],["r-basics.html", "2 R basics and workflows 2.1 Introducing Functions 2.2 Introduction to Objects 2.3 Workspace and working directory 2.4 RStudio projects 2.5 Saving Files and Protips", " 2 R basics and workflows Who is R? Why is R troubling PhD students?@AcademicChatter #AcademicTwitter&mdash; Dr. Marie Curie (@CurieDr) January 31, 2021 Well start our tour of R with a summary of how R code is used to interact with R via the console. In these notes, blocks of example R code are offset from the main text as shown below. Comments within code blocks start with a single hash #, the code itself has nothing consistent preceding it, and output from my R console is preceded by a double hash ##. You can copy and paste example code directly into your R console. Anything after the # will be ignored. # This is a comment within a block of R code. Comments # start with the hash sign and are ignored by R. The code # below will be interpreted by R once you paste or type it # into the console. x &lt;- c(4, 8, 15, 16, 23, 42) mean(x) # Only code after the hash is ignored #&gt; [1] 18 sd(x) #&gt; [1] 13.5 In the code above, were creating a short vector of scores in x and calculating its mean and standard deviation. Ok, if you havent opened up R/RStudio, do that now. Notice the default panes: Console (entire left) Environment/History (tabbed in upper right) Files/Plots/Packages/Help (tabbed in lower right) FYI: you can change the default location of the panes, among many other things: Customizing RStudio. Go into the Console, where we interact with the live R process. You should paste that example code in the console and verify that you get the same results. Note that code you enter at the console is preceded by the R prompt &gt;, whereas output printed in your console is not. 2.1 Introducing Functions x &lt;- c(4, 8, 15, 16, 23, 42) mean(x) #&gt; [1] 18 sd(x) #&gt; [1] 13.5 In the example code above (and in the previous section), the functions that get things done in R have names. To use them, we summon by name with parentheses enclosing any required information or instructions for how the functions should work. For example, we used the function c() to combine a set of numbers into a vector of scores. The information supplied to c() consisted of the scores themselves, separated by commas. mean() and sd() are functions for obtaining the mean and standard deviation of vectors of scores, like the ones in x. 2.2 Introduction to Objects x &lt;- c(4, 8, 15, 16, 23, 42) mean(x) #&gt; [1] 18 sd(x) #&gt; [1] 13.5 The second thing to notice in the example above is that data and results are saved to objects in R using the assignment operator &lt;-. We used the concatenate function to stick our numbers together in a set, c(4, 8, 15, 16, 23, 42), and then we assigned the result to have the name x. Objects created in this way can be accessed later on by their assigned name, for example, to find a mean or standard deviation. If we wanted to access it later, we could also save the mean of x to a new object. 2.2.0.1 Now you try! Make an assignment and then inspect the object you just created: x &lt;- 3 * 4 x #&gt; [1] 12 All R statements where you create objects  assignments  have this form: objectName &lt;- value and in my head I hear, e.g., x gets 12. Object names cannot start with a digit and cannot contain certain other characters such as a comma or a space. You will be wise to adopt a convention for demarcating words in names. i_use_snake_case other.people.use.periods evenOthersUseCamelCase Make another assignment: this_is_a_really_long_name &lt;- 2.5 To inspect this, try out RStudios completion facility: type the first few characters, press TAB, add characters until you disambiguate, then press return. Make another assignment: mason_rocks &lt;- 2 ^ 3 Lets try to inspect: masonrocks #&gt; Error in eval(expr, envir, enclos): object &#39;masonrocks&#39; not found masn_rocks #&gt; Error in eval(expr, envir, enclos): object &#39;masn_rocks&#39; not found Implicit contract with the computer / scripting language: Computer will do tedious computation for you. In return, you will be completely precise in your instructions. Typos matter. Case matters. mason_rocks #&gt; [1] 8 R has a mind-blowing collection of built-in functions that are accessed like so: functionName(arg1 = val1, arg2 = val2, and so on) Lets try using seq() which makes regular sequences of numbers and, while were at it, demo more helpful features of RStudio. Type se and hit TAB. A pop up shows you possible completions. Specify seq() by typing more to disambiguate or using the up/down arrows to select. Notice the floating tool-tip-type help that pops up, reminding you of a functions arguments. If you want even more help, press F1 as directed to get the full documentation in the help tab of the lower right pane. Now open the parentheses and notice the automatic addition of the closing parenthesis and the placement of cursor in the middle. Type the arguments 1, 10 and hit return. RStudio also exits the parenthetical expression for you. IDEs are great. seq(1, 10) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 The above also demonstrates something about how R resolves function arguments. You can always specify in name = value form. But if you do not, R attempts to resolve by position. So above, it is assumed that we want a sequence from = 1 that goes to = 10. Since we didnt specify step size, the default value of by in the function definition is used, which ends up being 1 in this case. For functions I call often, I might use this resolve by position for the first argument or maybe the first two. After that, I always use name = value. Make this assignment and notice similar help with quotation marks. yo &lt;- &quot;hello world&quot; If you just make an assignment, you dont get to see the value, so then youre tempted to immediately inspect. y &lt;- seq(1, 10) y #&gt; [1] 1 2 3 4 5 6 7 8 9 10 This common action can be shortened by surrounding the assignment with parentheses, which causes assignment and print to screen to happen. (y &lt;- seq(1, 10)) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 Not all functions have (or require) arguments: date() #&gt; [1] &quot;Sat Mar 06 17:16:52 2021&quot; Now look at your workspace  in the upper right pane. The workspace is where user-defined objects accumulate. You can also get a listing of these objects with commands: objects() #&gt; [1] &quot;check_quietly&quot; &quot;ds4p_urls&quot; #&gt; [3] &quot;install_quietly&quot; &quot;mason_rocks&quot; #&gt; [5] &quot;pretty_install&quot; &quot;sample_no_surprises&quot; #&gt; [7] &quot;shhh_check&quot; &quot;slide_url&quot; #&gt; [9] &quot;this_is_a_really_long_name&quot; &quot;x&quot; #&gt; [11] &quot;y&quot; &quot;yo&quot; ls() #&gt; [1] &quot;check_quietly&quot; &quot;ds4p_urls&quot; #&gt; [3] &quot;install_quietly&quot; &quot;mason_rocks&quot; #&gt; [5] &quot;pretty_install&quot; &quot;sample_no_surprises&quot; #&gt; [7] &quot;shhh_check&quot; &quot;slide_url&quot; #&gt; [9] &quot;this_is_a_really_long_name&quot; &quot;x&quot; #&gt; [11] &quot;y&quot; &quot;yo&quot; If you want to remove the object named y, you can do this: rm(y) To remove everything: rm(list = ls()) or click the broom in RStudios Environment pane. 2.3 Workspace and working directory One day you will need to quit R, go do something else and return to your analysis later. One day you will have multiple analyses going that use R and you want to keep them separate. One day you will need to bring data from the outside world into R and send numerical results and figures from R back out into the world. To handle these real life situations, you need to make two decisions: What about your analysis is real, i.e. will you save it as your lasting record of what happened? Where does your analysis live? 2.3.1 Workspace, .RData As a beginning R user, its OK to consider your workspace real. Very soon, I urge you to evolve to the next level, where you consider your saved R scripts as real. (In either case, of course the input data is very much real and requires preservation!) With the input data and the R code you used, you can reproduce everything. You can make your analysis fancier. You can get to the bottom of puzzling results and discover and fix bugs in your code. You can reuse the code to conduct similar analyses in new projects. You can remake a figure with different aspect ratio or save is as TIFF instead of PDF. You are ready to take questions. You are ready for the future. If you regard your workspace as real (saving and reloading all the time), if you need to redo analysis  youre going to either redo a lot of typing (making mistakes all the way) or will have to mine your R history for the commands you used. Rather than becoming an expert on managing the R history, a better use of your time and energy is to keep your good R code in a script for future reuse. Because it can be useful sometimes, note the commands youve recently run appear in the History pane. But you dont have to choose right now and the two strategies are not incompatible. Lets demo the save / reload the workspace approach. Upon quitting R, you have to decide if you want to save your workspace, for potential restoration the next time you launch R. Depending on your set up, R or your IDE, e.g. RStudio, will probably prompt you to make this decision. Quit R/RStudio, either from the menu, using a keyboard shortcut, or by typing q() in the Console. Youll get a prompt like this: Save workspace image to ~/.Rdata? Note where the workspace image is to be saved and then click Save. Using your favorite method, visit the directory where image was saved and verify there is a file named .RData. You will also see a file .Rhistory, holding the commands submitted in your recent session. Restart RStudio. In the Console you will see a line like this: [Workspace loaded from ~/.RData] indicating that your workspace has been restored. Look in the Workspace pane and youll see the same objects as before. In the History tab of the same pane, you should also see your command history. Youre back in business. This way of starting and stopping analytical work will not serve you well for long but its a start. 2.3.2 Working directory Any process running on your computer has a notion of its working directory. In R, this is where R will look, by default, for files you ask it to load. It also where, by default, any files you write to disk will go. Chances are your current working directory is the directory we inspected above, i.e. the one where RStudio wanted to save the workspace. You can explicitly check your working directory with: getwd() It is also displayed at the top of the RStudio console. As a beginning R user, its OK let your home directory or any other weird directory on your computer be Rs working directory. Very soon, I urge you to evolve to the next level, where you organize your projects into directories and, when working on project A, set Rs working directory to the associated directory. Although I do not recommend it, in case youre curious, you can set Rs working directory at the command line like so: setwd(&quot;~/myCoolProject&quot;) Although I do not recommend it, you can also use RStudios Files pane to navigate to a directory and then set it as working directory from the menu: Session &gt; Set Working Directory &gt; To Files Pane Location. (Youll see even more options there). Or within the Files pane, choose More and Set As Working Directory. But theres a better way. A way that also puts you on the path to managing your R work like an expert. 2.4 RStudio projects Keeping all the files associated with a project organized together  input data, R scripts, results, figures  is such a wise and common practice that RStudio has built-in support for this via its projects. Lets make one to use for the rest of this class. Do this: File &gt; New Project. The directory name you choose here will be the project name. Call it whatever you want (or follow me for convenience). I created a directory and, therefore RStudio project, called swc in my tmp directory, FYI. setwd(&quot;~/tmp/swc&quot;) Now check that the home directory for your project is the working directory of our current R process: getwd() I cant print my output here because this document itself does not reside in the RStudio Project we just created. Lets enter a few commands in the Console, as if we are just beginning a project: a &lt;- 2 b &lt;- -3 sig_sq &lt;- 0.5 x &lt;- runif(40) y &lt;- a + b * x + rnorm(40, sd = sqrt(sig_sq)) (avg_x &lt;- mean(x)) #&gt; [1] 0.589 write(avg_x, &quot;avg_x.txt&quot;) plot(x, y) abline(a, b, col = &quot;purple&quot;) dev.print(pdf, &quot;toy_line_plot.pdf&quot;) #&gt; png #&gt; 2 Lets say this is a good start of an analysis and your ready to start preserving the logic and code. Visit the History tab of the upper right pane. Select these commands. Click To Source. Now you have a new pane containing a nascent R script. Click on the floppy disk to save. Give it a name ending in .R or .r, I used toy-line.r and note that, by default, it will go in the directory associated with your project. Quit RStudio. Inspect the folder associated with your project if you wish. Maybe view the PDF in an external viewer. Restart RStudio. Notice that things, by default, restore to where we were earlier, e.g. objects in the workspace, the command history, which files are open for editing, where we are in the file system browser, the working directory for the R process, etc. These are all Good Things. Change some things about your code. Top priority would be to set a sample size n at the top, e.g. n &lt;- 40, and then replace all the hard-wired 40s with n. Change some other minor-but-detectable stuff, e.g. alter the sample size n, the slope of the line b,the color of the line  whatever. Practice the different ways to re-run the code: Walk through line by line by keyboard shortcut (Command+Enter) or mouse (click Run in the upper right corner of editor pane). Source the entire document  equivalent to entering source('toy-line.r') in the Console  by keyboard shortcut (Shift+Command+S) or mouse (click Source in the upper right corner of editor pane or select from the mini-menu accessible from the associated down triangle). Source with echo from the Source mini-menu. Visit your figure in an external viewer to verify that the PDF is changing as you expect. In your favorite OS-specific way, search your files for toy_line_plot.pdf and presumably you will find the PDF itself (no surprise) but also the script that created it (toy-line.r). This latter phenomenon is a huge win. One day you will want to remake a figure or just simply understand where it came from. If you rigorously save figures to file with R code and not ever ever ever the mouse or the clipboard, you will sing my praises one day. Trust me. 2.5 Saving Files and Protips It is traditional to save R scripts with a .R or .r suffix. Follow this convention unless you have some extraordinary reason not to. Comments start with one or more # symbols. Use them. RStudio helps you (de)comment selected lines with Ctrl+Shift+C (Windows and Linux) or Command+Shift+C (Mac). Clean out the workspace, i.e. pretend like youve just revisited this project after a long absence. The broom icon or rm(list = ls()). Good idea to do this, restart R (available from the Session menu), re-run your analysis to truly check that the code youre saving is complete and correct (or at least rule out obvious problems!). This workflow will serve you well in the future: Create an RStudio project for an analytical project Keep inputs there (well soon talk about importing) Keep scripts there; edit them, run them in bits or as a whole from there Keep outputs there (like the PDF written above) Avoid using the mouse for pieces of your analytical workflow, such as loading a dataset or saving a figure. Terribly important for reproducibility and for making it possible to retrospectively determine how a numerical table or PDF was actually produced (searching on local disk on filename, among .R files, will lead to the relevant script). Many long-time users never save the workspace, never save .RData files (Im one of them), never save or consult the history. Once/if you get to that point, there are options available in RStudio to disable the loading of .RData and permanently suppress the prompt on exit to save the workspace (go to Tools &gt; Options &gt; General). For the record, when loading data into R and/or writing outputs to file, you can always specify the absolute path and thereby insulate yourself from the current working directory. This method is rarely necessary when using RStudio projects. "],["refreshers-and-examples.html", "3 Refreshers and Examples 3.1 Some terms 3.2 Summary Statistics", " 3 Refreshers and Examples Youre now ready for a review of the introductory statistics that are prerequisite for the analyses that come later in this class. Statistics are important in measurement because they allow us to score and summarize the information collected with our tests and instruments. Theyre used to describe the reliability, validity, and predictive power of this information. Theyre also used to describe how well our test covers a domain of content or a network of constructs, including in relation to other content areas or constructs. We rely heavily on statistics for later modules 3.1 Some terms Well begin this review with some basic statistical terms. These ideas should be familiar to you from your research methods classes, but I figured that this information is useful in case youve gotten a little rusty. First, a variable is a set of values that can differ for different people. For example, we often measure variables such as age and gender. These words are italicized here to denote them as statistical variables, as opposed to words. The term variable is synonymous with quality, attribute, trait, or property. Constructs are also variables. Really, a variable is anything assigned to people that can potentially take on more than just a single constant value. As noted above, variables in R can be contained within simple vectors, for example, x, or they can be grouped together in a data.frame. Generic variables will be labeled in this book using capital letters, usually \\(X\\) and \\(Y\\). Here, \\(X\\) might represent a generic test score, for example, the total score across all the items in a test. It might also represent scores on a single item. Both are considered variables. The definition of a generic variable like \\(X\\) depends on the context in which it is defined. Indices can also be used to denote generic variables that are part of some sequence of variables. Most often this will be scores on items within a test, where, for example, \\(X_1\\) is the first item, \\(X_2\\) is the second, and \\(X_J\\) is the last, with \\(J\\) being the number of items in the test and \\(X_j\\) representing any given item. Subscripts can also be used to index individual people on a single variable. For example, test scores for a group of people could be denoted as \\(X_1\\), \\(X_2\\), \\(\\dots\\), \\(X_N\\), where \\(N\\) is the number of people and \\(X_i\\) represents the score for a generic person. Combining people and items, \\(X_{ij}\\) would be the score for person \\(i\\) on item \\(j\\). The number of people is denoted by \\(n\\) or sometimes \\(N\\). Typically, the lowercase \\(n\\) represents sample size and the uppercase \\(N\\) represents the population, however, the two are often used interchangeably. Greek and Arabic letters are used for other sample and population statistics. The sample mean is denoted by \\(m\\) and the population mean by \\(\\mu\\), the standard deviation is \\(s\\) or \\(\\sigma\\), variance is \\(s^2\\) or \\(\\sigma^2\\), and correlation is \\(r\\) or \\(\\rho\\). Note that the mean and standard deviation are sometimes abbreviated as \\(M\\) and \\(SD\\). Note also that distinctions between sample and population values often arent necessary, in which case the population terms are used. If a distinction is necessary, it will be identified. Finally, you may see named subscripts added to variable names and other terms, for example, \\(M_{control}\\) might denote the mean of a control group. These subscripts depend on the situation and must be interpreted in context. 3.2 Summary Statistics Descriptive and inferential are terms that refer to two general uses of statistics. These uses differ based on whether or not an inference is made from properties of a sample of data to parameters for an unknown population. Descriptive statistics, or descriptives, are used simply to explore and describe certain features of distributions. For example, the mean and variance are statistics identifying the center of and variability in a distribution. These and other statistics are used inferentially when an inference is made to a population Descriptives are not typically used to answer research questions or inform decision making. Instead, inferential statistics are more appropriate for these less exploratory and more confirmatory results. Inferential statistics involve an inference to a parameter or a population value. The quality of this inference is gauged using statistical tests that index the error associated with our estimates. In this review were focusing on descriptive statistics. Later well consider some inferential applications. The describe() function in the psych package returns basic descriptive statistics that are often useful for psychometrics, including the mean, median, standard deviation (sd), skewness (skew), kurtosis (kurt), minimum (min), and maximum (max), as well as some others Im forgetting. The describe function in the psych package is meant to produce the most frequently requested stats in psychometric and psychology studies, and to produce them in an easy to read data.frame. If a grouping variable is called for in formula mode, it will also call describeBy to the processing. The results from describe can be used in graphics functions (e.g., error.crosses). The range statistics (min, max, range) are most useful for data checking to detect coding errors, and should be found in early analyses of the data. Although describe will work on data frames as well as matrices, it is important to realize that for data frames, descriptive statistics will be reported only for those variables where this makes sense (i.e., not for alphanumeric data). If the check option is TRUE, variables that are categorical or logical are converted to numeric and then described. These variables are marked with an * in the row name. This is somewhat slower. Note that in the case of categories or factors, the numerical ordering is not necessarily the one expected. For instance, if education is coded high school, some college , finished college, then the default coding will lead to these as values of 2, 3, 1. Thus, statistics for those variables marked with * should be interpreted cautiously (if at all). "],["r-code-for-module-5.html", "4 R Code for Module 5 4.1 Simulate a constant true score 4.2 PISA total reading scores with simulated error and true scores based on CTT 4.3 Reliability and unreliability Illustrated", " 4 R Code for Module 5 4.1 Simulate a constant true score In this example, I simulate a constant true score, and randomly varying error scores from a normal population with mean 0 and SD 1 Note, set.seed() gives R a starting point for generating random numbers, so we can get the same results on different computers You should check the mean and SD of E and X. Creating a histogram of X might be interesting too library(tidyverse) set.seed(160416) myt &lt;- 20 mye &lt;- rnorm(1000, mean = 0, sd = 1) myx &lt;- myt + mye df=data.frame(myx,myt,mye) mean(myx); sd(myx) #&gt; [1] 20 #&gt; [1] 1.03 p &lt;- ggplot(df, aes(x=myx)) + geom_histogram() p #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 4.2 PISA total reading scores with simulated error and true scores based on CTT ## Libraries #install.packages(&quot;devtools&quot;) #devtools::install_github(&quot;talbano/epmr&quot;) library(epmr) #&gt; #&gt; Attaching package: &#39;epmr&#39; #&gt; The following object is masked from &#39;package:psych&#39;: #&gt; #&gt; skew library(ggplot2) ritems &lt;- c(&quot;r414q02&quot;, &quot;r414q11&quot;, &quot;r414q06&quot;, &quot;r414q09&quot;, &quot;r452q03&quot;, &quot;r452q04&quot;, &quot;r452q06&quot;, &quot;r452q07&quot;, &quot;r458q01&quot;, &quot;r458q07&quot;, &quot;r458q04&quot;) rsitems &lt;- paste0(ritems, &quot;s&quot;) BEL_PISA09=PISA09[PISA09$cnt == &quot;BEL&quot;, rsitems] xscores &lt;- rowSums(BEL_PISA09, na.rm = TRUE) Simulate error scores based on known SEM of 1.4, which well calculate later, then create true scores True scores are truncated to fall between 0 and 11 using setrange() escores &lt;- rnorm(length(xscores), 0, 1.4) tscores &lt;- setrange(xscores - escores, y = xscores) Combine in a data frame and create a scatterplot scores &lt;- data.frame(x1 = xscores, t = tscores, e = escores) ggplot(scores, aes(x1, t)) + geom_point(position = position_jitter(w = .3)) + geom_abline(col = &quot;blue&quot;) 4.3 Reliability and unreliability Illustrated Here we have simulated scores for a new form of the reading test called y. Note that rho is the made up reliability, which is set to 0.80, and x is the original reading total scores. Form y, which is slightly easier than x, has a mean of 6 and SD of 3. xysim &lt;- rsim(rho = .8, x = scores$x1, meany = 6, sdy = 3) scores$y &lt;- round(setrange(xysim$y, scores$x1)) ggplot(scores, aes(x1, y)) + geom_point(position = position_jitter(w = .3, h = .3)) + geom_abline(col = &quot;blue&quot;) Figure 4.1: PISA total reading scores and scores on a simulated second form of the reading test. "],["installing-epmr.html", "5 Installing epmr", " 5 Installing epmr A lot of the examples Im using in the class require the epmr package. In theory, it should be really easy to install, using the following code. # Will install devtools if not already installed if (!require(&quot;devtools&quot;)) install.packages(&quot;devtools&quot;) devtools::install_github(&quot;talbano/epmr&quot;) It will install devtools, if you dont have it already and then compile the current version of the package for you. However, if your computer doesnt already have a compiler, you might have some trouble. If you have trouble installing the epmr package, you are not alone. I, Prof. Mason, am working on resolving the issue. To install epmr package directly from github, you need devtools and a working development environment (a.k.a. an R friendly compiler). The specific compiler will depend on your operating system. Windows: Install Rtools. Mac: Install Xcode from the Mac App Store. Linux: Install a compiler and various development libraries (details vary across different flavors of Linux). Try installing the compiler that corresponds to your operating system. And then try installing directly from github again. # Will install devtools if not already installed if (!require(&quot;devtools&quot;)) install.packages(&quot;devtools&quot;) devtools::install_github(&quot;talbano/epmr&quot;) If the compilation doesnt work, dont despair! Ive compiled a version that should work. First download this file epmr_0.0.0.9000.tar.gz from github, and place it in your working directory. (Here is the direct download link:) Then, install that package directly, using this code. install.packages(&quot;epmr_0.0.0.9000.tar.gz&quot;, repos = NULL, type = &quot;source&quot;) If that doesnt work, please try using the Rstudio GUI, as illustrated below. Open the tools dropdown menu, and click Install Packages Then change the install from setting to Install from package archive Then select browse for file and locate the archive epmr_0.0.0.9000.tar.gz Then select that archive, and you should see something like the image below Press Install. This action should begin the installation process. If you still cannot get the package to install, please let me know as soon as possible. That way, I can help you during regular business hours. (In other words, dont wait until the last minute) "],["statistical-definition-of-reliability-.html", "6 Statistical definition of reliability. 6.1 Estimating reliability 6.2 Split-half method 6.3 Spearman Brown 6.4 SEM", " 6 Statistical definition of reliability. In CTT, reliability is defined as the proportion of variability in \\(X\\) that is due to variability in true scores \\(T\\): \\[\\begin{equation} r = \\frac{\\sigma^2_T}{\\sigma^2_X}. \\tag{6.1} \\end{equation}\\] 6.1 Estimating reliability One indirect estimate made possible by CTT is the correlation between scores on two forms of the same test: \\[\\begin{equation} r = \\rho_{X_1 X_2} = \\frac{\\sigma_{X_1 X_2}}{\\sigma_{X_1} \\sigma_{X_2}}. \\tag{6.2} \\end{equation}\\] 6.2 Split-half method The split-half method takes scores on a single test form, and separates them into scores on two halves of the test, which are treated as separate test forms. The correlation between these two halves then represents an indirect estimate of reliability, based on Equation (6.1). # Split half correlation, assuming we only had scores on # one test form # With an odd number of reading items, one half has 5 # items and the other has 6 library(epmr) xsplit1 &lt;- rowSums(PISA09[PISA09$cnt == &quot;BEL&quot;, rsitems[1:5]]) xsplit2 &lt;- rowSums(PISA09[PISA09$cnt == &quot;BEL&quot;, rsitems[6:11]]) cor(xsplit1, xsplit2, use = &quot;complete&quot;) #&gt; [1] 0.625 6.3 Spearman Brown The Spearman-Brown formula was originally used to correct for the reduction in reliability that occurred when correlating two test forms that were only half the length of the original test. In theory, reliability will increase as we add items to a test. Thus, Spearman-Brown is used to estimate, or predict, what the reliability would be if the half-length tests were made into full-length tests. # sb_r() in the epmr package uses the Spearman-Brown # formula to estimate how reliability would change when # test length changes by a factor k # If test length were doubled, k would be 2 sb_r(r = cor(xsplit1, xsplit2, use = &quot;complete&quot;), k = 2) #&gt; [1] 0.769 The Spearman-Brown reliability, \\(r_{new}\\), is estimated as a function of whats labeled here as the old reliability, \\(r_{old}\\), and the factor by which the length of \\(X\\) is predicted to change, \\(k\\): \\[\\begin{equation} r_{new} = \\frac{kr_{old}}{(k - 1)r_{old} + 1}. \\tag{6.3} \\end{equation}\\] Again, \\(k\\) is the factor by which the test length is increased or decreased. It is equal to the number of items in the new test divided by the number of items in the original test. Multiply \\(k\\) by the old reliability, and then divided the result by \\((k - 1)\\) times the old reliability, plus 1. The epmr package contains sb_r(), a simple function for estimating the Spearman-Brown reliability. 6.4 SEM Typically, were more interested in how the unreliability of a test can be expressed in terms of the available observed variability. Thus, we multiply the unreliable proportion of variance by the standard deviation of \\(X\\) to obtain the SEM: \\[\\begin{equation} SEM = \\sigma_X\\sqrt{1 - r}. \\tag{6.4} \\end{equation}\\] Confidence intervals for PISA09 can be estimated in the same way. First, we choose a measure of reliability, find the SD of observed scores, and obtain the corresponding SEM. Then, we can find the CI, which gives us the expected amount of uncertainty in our observed scores due to random measurement error. Here, were calculating SEM and the CI using alpha, but other reliability estimates would work as well. Figure 6.1 shows the 11 possible PISA09 reading scores in order, with error bars based on SEM for students in Belgium. # Get alpha and SEM for students in Belgium bela &lt;- coef_alpha(PISA09[PISA09$cnt == &quot;BEL&quot;, rsitems]) bela_alpha=bela$alpha # The sem function from epmr also overlaps with sem from # another package so we&#39;re spelling it out here in long # form belsem &lt;- epmr::sem(r = bela_alpha, sd = sd(scores$x1, na.rm = TRUE)) # Plot the 11 possible total scores against themselves # Error bars are shown for 1 SEM, giving a 68% confidence # interval and 2 SEM, giving the 95% confidence interval # x is converted to factor to show discrete values on the # x-axis beldat &lt;- data.frame(x = 1:11, sem = belsem) ggplot(beldat, aes(factor(x), x)) + geom_errorbar(aes(ymin = x - sem * 2, ymax = x + sem * 2), col = &quot;violet&quot;) + geom_errorbar(aes(ymin = x - sem, ymax = x + sem), col = &quot;yellow&quot;) + geom_point() Figure 6.1: The PISA09 reading scale shown with 68 and 95 percent confidence intervals around each point. Figure 6.1 helps us visualize the impact of unreliable measurement on score comparisons. For example, note that the top of the 95% confidence interval for \\(X\\) of 2 extends nearly to 5 points, and thus overlaps with the CI for adjacent scores 3 through 7. It isnt until \\(X\\) of 8 that the CI no longer overlap. With a CI of belsem 1.425, were 95% confident that students with observed scores differing at least by belsem * 4 5.7 have different true scores. Students with observed scores closer than belsem * 4 may actually have the same true scores. "],["interpreting-reliability-and-unreliability.html", "7 Interpreting reliability and unreliability", " 7 Interpreting reliability and unreliability There are no agreed-upon standards for interpreting reliability coefficients. Reliability is bound by 0 on the lower end and 1 at the upper end, because, by definition, the amount of true variability can never be less or more than the total available variability in \\(X\\). Higher reliability is clearly better, but cutoffs for acceptable levels of reliability vary for different fields, situations, and types of tests. The stakes of a test are an important consideration when interpreting reliability coefficients. The higher the stakes, the higher we expect reliability to be. Otherwise, cutoffs depend on the particular application. In general, reliabilities for educational and psychological tests can be interpreted using scales like the ones presented in Table 7.1. With medium-stakes tests, a reliability of 0.70 is sometimes considered minimally acceptable, 0.80 is decent, 0.90 is quite good, and anything above 0.90 is excellent. High stakes tests should have reliabilities at or above 0.90. Low stakes tests, which are often simpler and shorter than higher-stakes ones, often have reliabilities as low as 0.70. These are general guidelines, and interpretations can vary considerably by test. Remember that the cognitive measures in PISA would be considered low-stakes at the student level. A few additional considerations are necessary when interpreting coefficient alpha. First, alpha assumes that all items measure the same single construct. Items are also assumed to be equally related to this construct, that is, they are assumed to be parallel measures of the construct. When the items are not parallel measures of the construct, alpha is considered a lower-bound estimate of reliability, that is, the true reliability for the test is expected to be higher than indicated by alpha. Finally, alpha is not a measure of dimensionality. It is frequently claimed that a strong coefficient alpha supports the unidimensionality of a measure. However, alpha does not index dimensionality. It is impacted by the extent to which all of the test items measure a single construct, but it does not necessarily go up or down as a test becomes more or less unidimensional. Table 7.1: General Guidelines for Interpreting Reliability Coefficients Reliability High Stakes Interpretation Low Stakes Interpretation \\(\\geq 0.90\\) Excellent Excellent \\(0.80 \\leq r &lt; 0.90\\) Good Excellent \\(0.70 \\leq r &lt; 0.80\\) Acceptable Good \\(0.60 \\leq r &lt; 0.70\\) Borderline Acceptable \\(0.50 \\leq r &lt; 0.60\\) Low Borderline \\(0.20 \\leq r &lt; 0.50\\) Unacceptable Low \\(0.00 \\leq r &lt; 0.20\\) Unacceptable Unacceptable "],["reliability-study-designs.html", "8 Reliability study designs 8.1 Interrater reliability 8.2 Summary", " 8 Reliability study designs Now that weve established the more common estimates of reliability and unreliability, we can discuss the four main study designs that allow us to collect data for our estimates. These designs are referred to as internal consistency, equivalence, stability, and equivalence/stability designs. Each design produces a corresponding type of reliability that is expected to be impacted by different sources of measurement error. The four standard study designs vary in the number of test forms and the number of testing occasions involved in the study. Until now, weve been talking about using two test forms on two separate administrations. This study design is found in the lower right corner of Table 8.1, and it provides us with an estimate of equivalence (for two different forms of a test) and stability (across two different administrations of the test). This study design has the potential to capture the most sources of measurement error, and it can thus produce the lowest estimate of reliability, because of the two factors involved. The more time that passes between administrations, and as two test forms differ more in their content and other features, the more error we would expect to be introduced. On the other hand, as our two test forms are administered closer in time, we move from the lower right corner to the upper right corner of Table 8.1, and our estimate of reliability captures less of the measurement error introduced by the passage of time. Were left with an estimate of the equivalence between the two forms. As our test forms become more and more equivalent, we eventually end up with the same test form, and we move to the first column in Table 8.1, where one of two types of reliability is estimated. First, if we administer the same test twice with time passing between administrations, we have an estimate of the stability of our measurement over time. Given that the same test is given twice, any measurement error will be due to the passage of time, rather than differences between the test forms. Second, if we administer one test only once, we no longer have an estimate of stability, and we also no longer have an estimate of reliability that is based on correlation. Instead, we have an estimate of what is referred to as the internal consistency of the measurement. This is based on the relationships among the test items themselves, which we treat as miniature alternate forms of the test. The resulting reliability estimate is impacted by error that comes from the items themselves being unstable estimates of the construct of interest. Table 8.1: Four Main Reliability Study Designs 1 Form 2 Forms 1 Occasion Internal Consistency Equivalence 2 Occasions Stability Equivalence &amp; Stability Internal consistency reliability is estimated using either coefficient alpha or split-half reliability. All the remaining cells in Table 8.1 involve estimates of reliability that are based on correlation coefficients. Table 8.1 contains four commonly used reliability study designs. There are others, including designs based on more than two forms or more than two occasions, and designs involving scores from raters, discussed below. 8.1 Interrater reliability It was like listening to three cats getting strangled in an alley.  Simon Cowell, disparaging a singer on American Idol Interrater reliability can be considered a specific instance of reliability where inconsistencies are not attributed to differences in test forms, test items, or administration occasions, but to the scoring process itself, where humans, or in some cases computers, contribute as raters. Measurement with raters often involves some form of performance assessment, for example, a stage performance within a singing competition. Judgment and scoring of such a performance by raters introduces additional error into the measurement process. Interrater reliability allows us to examine the negative impact of this error on our results. Note that rater error is another factor or facet in the measurement process. Because it is another facet of measurement, raters can introduce error above and beyond error coming from sampling of items, differences in test forms, or the passage of time between administrations. This is made explicit within generalizability theory, discussed below. Some simpler methods for evaluating interrater reliability are introduced first. 8.1.1 Proportion agreement The proportion of agreement is the simplest measure of interrater reliability. It is calculated as the total number of times a set of ratings agree, divided by the total number of units of observation that are rated. The strengths of proportion agreement are that it is simple to calculate and it can be used with any type of discrete measurement scale. The major drawbacks are that it doesnt account for chance agreement between ratings, and it only utilizes the nominal information in a scale, that is, any ordering of values is ignored. To see the effects of chance, lets simulate scores from two judges where ratings are completely random, as if scores of 0 and 1 are given according to the flip of a coin. Suppose 0 is tails and 1 is heads. In this case, we would expect two raters to agree a certain proportion of the time by chance alone. The table() function creates a cross-tabulation of frequencies, also known as a crosstab. Frequencies for agreement are found in the diagonal cells, from upper left to lower right, and frequencies for disagreement are found everywhere else. # Simulate random coin flips for two raters # runif() generates random numbers from a uniform # distribution flip1 &lt;- round(runif(30)) flip2 &lt;- round(runif(30)) table(flip1, flip2) #&gt; flip2 #&gt; flip1 0 1 #&gt; 0 5 8 #&gt; 1 9 8 Lets find the proportion agreement for the simulated coin flip data. The question were answering is, how often did the coin flips have the same value, whether 0 or 1, for both raters across the 30 tosses? The crosstab shows this agreement in the first row and first column, with raters both flipping tails 5 times, and in the second row and second column, with raters both flipping heads 8 times. We can add these up to get 13, and divide by \\(n = 30\\) to get the percentage agreement. Data for the next few examples were simulated to represent scores given by two raters with a certain correlation, that is, a certain reliability. Thus, agreement here isnt simply by chance. In the population, scores from these raters correlated at 0.90. The score scale ranged from 0 to 6 points, with means set to 4 and 3 points for the raters 1 and 2, and SD of 1.5 for both. Well refer to these as essay scores, much like the essay scores on the analytical writing section of the GRE. Scores were also dichotomized around a hypothetical cut score of 3, resulting in either a Fail or Pass. # Simulate essay scores from two raters with a population # correlation of 0.90, and slightly different mean scores, # with score range 0 to 6 # Note the capital T is an abbreviation for TRUE essays &lt;- rsim(100, rho = .9, meanx = 4, meany = 3, sdx = 1.5, sdy = 1.5, to.data.frame = T) colnames(essays) &lt;- c(&quot;r1&quot;, &quot;r2&quot;) essays &lt;- round(setrange(essays, to = c(0, 6))) # Use a cut off of greater than or equal to 3 to determine # pass versus fail scores # ifelse() takes a vector of TRUEs and FALSEs as its first # argument, and returns here &quot;Pass&quot; for TRUE and &quot;Fail&quot; # for FALSE essays$f1 &lt;- factor(ifelse(essays$r1 &gt;= 3, &quot;Pass&quot;, &quot;Fail&quot;)) essays$f2 &lt;- factor(ifelse(essays$r2 &gt;= 3, &quot;Pass&quot;, &quot;Fail&quot;)) table(essays$f1, essays$f2) #&gt; #&gt; Fail Pass #&gt; Fail 19 0 #&gt; Pass 27 54 The upper left cell in the table() output above shows that for 19 individuals, the two raters both gave Fail. In the lower right cell, the two raters both gave Pass 54 times. Together, these two totals represent the agreement in ratings, 73 . The other cells in the table contain disagreements, where one rater said Pass but the other said Fail. Disagreements happened a total of 27 times. Based on these totals, what is the proportion agreement in the pass/fail ratings? Table 8.2 shows the full crosstab of raw scores from each rater, with scores from rater 1 (essays$r1) in rows and rater 2 (essays$r2) in columns. The bunching of scores around the diagonal from upper left to lower right shows the tendency for agreement in scores. Table 8.2: Crosstab of Scores From Rater 1 in Rows and Rater 2 in Columns 0 1 2 3 4 5 6 0 1 1 0 0 0 0 0 1 1 2 0 0 0 0 0 2 5 8 1 0 0 0 0 3 0 6 11 2 1 0 0 4 0 0 9 9 10 0 0 5 0 0 1 4 6 3 1 6 0 0 0 2 3 3 10 Proportion agreement for the full rating scale, as shown in Table 8.2, can be calculated by summing the agreement frequencies within the diagonal elements of the table, and dividing by the total number of people. # Pull the diagonal elements out of the crosstab with # diag(), sum them, and divide by the number of people sum(diag(table(essays$r1, essays$r2))) / nrow(essays) #&gt; [1] 0.29 Finally, lets consider the impact of chance agreement between one of the hypothetical human raters and a monkey who randomly applies ratings, regardless of the performance that is demonstrated, as with a coin toss. # Randomly sample from the vector c(&quot;Pass&quot;, &quot;Fail&quot;), # nrow(essays) times, with replacement # Without replacement, we&#39;d only have 2 values to sample # from monkey &lt;- sample(c(&quot;Pass&quot;, &quot;Fail&quot;), nrow(essays), replace = TRUE) table(essays$f1, monkey) #&gt; monkey #&gt; Fail Pass #&gt; Fail 10 9 #&gt; Pass 38 43 The results show that the hypothetical rater agrees with the monkey 53 percent of the time. Because we know that the monkeys ratings were completely random, we know that this proportion agreement is due entirely to chance. 8.1.2 Kappa agreement Proportion agreement is useful, but because it does not account for chance agreement, it should not be used as the only measure of interrater consistency. Kappa agreement is simply an adjusted form of proportion agreement that takes chance agreement into account. Equation (8.1) contains the formula for calculating kappa for two raters. \\[\\begin{equation} \\kappa = \\frac{P_o - P_c}{1 - P_c} \\tag{8.1} \\end{equation}\\] To obtain kappa, we first calculate the proportion of agreement, \\(P_o\\), as we did with the proportion agreement. This is calculated as the total for agreement divided by the total number of people being rated. Next we calculate the chance agreement, \\(P_c\\), which involves multiplying the row and column proportions (row and column totals divided by the total total) from the crosstab and then summing the result, as shown in Equation (8.2). \\[\\begin{equation} P_c = P_{first-row}P_{first-col} + P_{next-row}P_{next-col} + \\dots + P_{last-row}P_{last-col} \\tag{8.2} \\end{equation}\\] You do not need to commit Equations (8.1) and (8.2) to memory. Instead, theyre included here to help you understand that kappa involves removing chance agreement from the observed agreement, and then dividing this observed non-chance agreement by the total possible non-chance agreement, that is, \\(1 - P_c\\). The denominator for the kappa equation contains the maximum possible agreement beyond chance, and the numerator contains the actual observed agreement beyond chance. So, the maximum possible kappa is 1.0. In theory, we shouldnt ever observe less agreement than than expected by chance, which means that kappa should never be negative. Technically it is possible to have kappa below 0. When kappa is below 0, it indicates that our observed agreement is below what wed expect due to chance. Kappa should also be no larger than proportion agreement. In the example data, the proportion agreement decreased from 0.29 to 0.159 for kappa. A weighted version of the kappa index is also available. Weighted kappa let us reduce the negative impact of partial disagreements relative to more extreme disagreements in scores, by taking into account the ordinal nature of a score scale. For example, in Table 8.2, notice that only the diagonal elements of the crosstab measure perfect agreement in scores, and all other elements measure disagreements, even the ones that are close together like 2 and 3. With weighted kappa, we can give less weight to these smaller disagreements and more weight to larger disagreements such as scores of 0 and 6 in the lower left and upper right of the table. This weighting ends up giving us a higher agreement estimate. Here, we use the function astudy() from epmr to calculate proportion agreement, kappa, and weighted kappa indices. Weighted kappa gives us the highest estimate of agreement. Refer to the documentation for astudy() to see how the weights are calculated. # Use the astudy() function from epmr to measure agreement astudy(essays[, 1:2]) #&gt; agree kappa wkappa #&gt; 0.290 0.159 0.479 8.1.3 Pearson correlation The Pearson correlation coefficient, introduced above for CTT reliability, improves upon agreement indices by accounting for the ordinal nature of ratings without the need for explicit weighting. The correlation tells us how consistent raters are in their rank orderings of individuals. Rank orderings that are closer to being in agreement are automatically given more weight when determining the overall consistency of scores. The main limitation of the correlation coefficient is that it ignores systematic differences in ratings when focusing on their rank orders. This limitation has to do with the fact that correlations are oblivious to linear transformations of score scales. We can modify the mean or standard deviation of one or both variables being correlated and get the same result. So, if two raters provide consistently different ratings, for example, if one rater is more forgiving overall, the correlation coefficient can still be high as long as the rank ordering of individuals does not change. This limitation is evident in our simulated essay scores, where rater 2 gave lower scores on average than rater 1. If we subtract 1 point from every score for rater 2, the scores across raters will be more similar, as shown in Figure 8.1, but we still get the same interrater reliability. # Certain changes in descriptive statistics, like adding # constants won&#39;t impact correlations cor(essays$r1, essays$r2) #&gt; [1] 0.854 dstudy(essays[, 1:2]) #&gt; #&gt; Descriptive Study #&gt; #&gt; mean median sd skew kurt min max n na #&gt; r1 3.86 4 1.49 -0.270 2.48 0 6 100 0 #&gt; r2 2.88 3 1.72 0.242 2.15 0 6 100 0 cor(essays$r1, essays$r2 + 1) #&gt; [1] 0.854 A systematic difference in scores can be visualized by a consistent vertical or horizontal shift in the points within a scatter plot. Figure 8.1 shows that as scores are shifted higher for rater 2, they are more consistent with rater 1 in an absolute sense, despite the fact that the underlying linear relationship remains unchanged. # Comparing scatter plots ggplot(essays, aes(r1, r2)) + geom_point(position = position_jitter(w = .1, h = .1)) + geom_abline(col = &quot;blue&quot;) ggplot(essays, aes(r1, r2 + 1)) + geom_point(position = position_jitter(w = .1, h = .1)) + geom_abline(col = &quot;blue&quot;) Figure 8.1: Scatter plots of simulated essay scores with a systematic difference around 0.5 points. Is it a problem that the correlation ignores systematic score differences? Can you think of any real-life situations where it wouldnt be cause for concern? A simple example is when awarding scholarships or giving other types of awards or recognition. In these cases consistent rank ordering is key and systematic differences are less important because the purpose of the ranking is to identify the top candidate. There is no absolute scale on which subjects are rated. Instead, they are rated in comparison to one another. As a result, a systematic difference in ratings would technically not matter. 8.2 Summary This chapter provided an overview of reliability within the frameworks of CTT, for items and test forms, for reliability study designs with multiple facets. After a general definition of reliability in terms of consistency in scores, the CTT model was introduced, and two commonly used indices of CTT reliability were discussed: correlation and coefficient alpha. Reliability was then presented as it relates to consistency of scores from raters. Inter-rater agreement indices were compared, along with the correlation coefficient. "],["validity.html", "9 Validity 9.1 Objectives 9.2 Overview of validity 9.3 Content validity 9.4 Criterion validity 9.5 Construct validity 9.6 Unified validity and threats 9.7 Summary", " 9 Validity Validity has long been one of the major deities in the pantheon of the psychometrician. It is universally praised, but the good works done in its name are remarkably few.  Robert Ebel As noted by Ebel (1961), validity is universally considered the most important feature of a testing program. Validity encompasses everything relating to the testing process that makes score inferences useful and meaningful. All of the topics covered in this course, provide evidence supporting the validity of scores. Scores that are consistent and based on items written according to specified content standards with appropriate levels of difficulty and discrimination are more useful and meaningful than scores that do not have these qualities. Correct measurement, sound test construction, reliability, and certain item properties are thus all prerequisites for validity. These notes begin with a definition of validity and some related terms. After defining validity, three common sources of validity evidence are discussed: test content via whats referred to as a test blueprint or test outline, relationships with criterion variables, and theoretical models of the construct being measured. These three sources of validity evidence are then discussed within a unified view of validity. Finally, threats to validity are addressed. 9.1 Objectives Learning objectives connected to these notes Define validity in terms of test score interpretation and use, and identify and describe examples of this definition in context. Compare and contrast three main types of validity evidence (content, criterion, and construct), with examples of how each type is established, including the validation process involved with each. Explain the structure and function of a test blueprint, and how it is used to provide evidence of content validity. Calculate and interpret a validity coefficient, describing what it represents and how it supports criterion validity. Describe how unreliability can attenuate a correlation, and how to correct for attenuation in a validity coefficient. Identify appropriate sources of validity evidence for given testing applications and describe how certain sources are more appropriate than others for certain applications. Describe the unified view of validity and how it differs from and improves upon the traditional view of validity. Identify threats to validity, including features of a test, testing process, or score interpretation or use, that impact validity. Consider, for example, the issues of content underrepresentation and misrepresentation, and construct irrelevant variance. R analysis in this chapter is minimal. We&#39;ll run correlations and make adjustments to them using the base R functions, and we&#39;ll simulate scores using epmr. ```r # R setup for this chapter library(&quot;epmr&quot;) # Functions we&#39;ll use # cor() from the base package # rsim() from epmr to simulate scores 9.2 Overview of validity 9.2.1 Definitions Suppose you are conducting a research study on the efficacy of a reading intervention. Scores on a reading test will be compared for a treatment group who participated in the intervention and a control group who did not. A statistically significant difference in mean reading scores for the two groups will be taken as evidence of an effective intervention. This is an inferential use of statistics, as discussed in Chapter ??. In measurement, we step back and evaluate the extent to which our mean scores for each group accurately measure what they are intended to measure. On the surface, the means themselves may differ. But if neither mean actually captures average reading ability, our results are misleading, and our intervention may not actually be effective. Instead, it may appear effective because of systematic or random error in our measurements. Reliability, from Chapter ??, focuses on the consistency of measurement. With reliability, we estimate the amount of variability in scores that can be attributed to a reliable source, and, conversely, the variability that can be attributed to an unreliable source, that is, random error. While reliability is useful, it does not tell us whether that reliable source of variability is the source we hope it is. This is the job of validity. With validity, we additionally examine the quality of our items as individual components of the target construct. We examine other sources of variability in our scores, such as item and test bias. We also examine relationships between scores on our items and other measures of the target construct. The Standards for Educational and Psychological Testing (AERA, APA, and NCME 1999) define validity as the degree to which evidence and theory support the interpretations of test scores entailed by the proposed uses of a test. This definition is simple, but very broad, encompassing a wide range of evidence and theory. Well focus on three specific types of validity evidence, evidence based on test content, other measures, and theoretical models. Recent literature on validity theory has clarified that tests and even test scores themselves are not valid or invalid. Instead, only score inferences and interpretations are valid or invalid (e.g., Kane 2013). Tests are then described as being valid only for a particular use. This is a simple distinction in the definition of validity, but some authors continue to highlight it. Referring to a test or test score as valid implies that it is valid for any use, even though this is likely not the case. Shorthand is sometimes used to refer to tests themselves as valid, because it is simpler than distinguishing between tests, uses, and interpretations. However, the assumption is always that validity only applies to a specific test use and not broadly to the test itself. Finally, Kane (2013) also clarifies that validity is a matter of degree. It is establish incrementally through an accumulation of supporting evidence. Validity is not inherent in a test, and it is not simply declared to exist by a test developer. Instead, data are collected and research is conducted to establish evidence supporting a test for a particular use. As this evidence builds, so does our confidence that test scores can be used for their intended purpose. 9.2.2 Validity examples To evaluate the proposed score interpretations and uses for a test, and the extent to which they are valid, we should first examine the purpose of the test itself. As discussed in Chapters ?? and ??, a good test purpose articulates key information about the test, including what it measures (the construct), for whom (the intended population), and why (for what purpose). The question then becomes, given the quality of its contents, how they were constructed, and how they are implemented, is the test valid for this purpose? As a first example, lets return to the test of early literacy introduced in Chapter ??. Documentation for the test (www.myigdis.com) claims that, myIGDIs are a comprehensive set of assessments for monitoring the growth and development of young children. myIGDIs are easy to collect, sensitive to small changes in childrens achievement, and mark progress toward a long-term desired outcome. For these reasons, myIGDIs are an excellent choice for monitoring English Language Learners and making more informed Special Education evaluations. Different types of validity evidence would be needed to support the claims made for the IGDI measures. The comprehensiveness of the measures could be documented via test outlines that are based on a broad but well-defined content domain, and that are vetted by content experts, including teachers. Multiple test forms would be needed to monitor growth, and the quality and equivalence of these forms could be established using appropriate reliability estimates and measurement scaling techniques, such as Rasch modeling. Ease of data collection could be documented by the simplicity and clarity of the test manual and administration instructions, which could be evaluated by users, and the length and complexity of the measures. The sensitivity of the measures to small changes in achievement and their relevance to long-term desired outcomes could be documented using statistical relationships between IGDI scores and other measures of growth and achievement within a longitudinal study. Finally, all of these sources of validity evidence would need to be gathered both for English Language Learners and other target groups in special education. These various forms of information all fit into the sources of validity evidence discussed below. As a second example, consider a test construct that interests you. What construct are you interested in measuring? Perhaps it is one construct measured within a larger research study? How could you measure this construct? What type of test are you going to use? And what types of score(s) from the test will be used to support decision making? Next, consider who is going to take this test. Be as specific as possible when identifying your target population, the individuals that your work or research focuses on. Finally, consider why these people are taking your test. What are you going to do with the test scores? What are your proposed score interpretations and uses? Having defined your test purpose, consider what type of evidence would prove that the test is doing what you intend it to do, or that the score interpretations and uses are what you intend them to be. What information would support your test purpose? 9.2.3 Sources of validity evidence The information gathered to support a test purpose, and establish validity evidence for the intended uses of a test, is often categorized into three main areas of validity evidence. These are content, criterion, and construct validity. Nowadays, these are referred to as sources of validity evidence, where content focuses on the test content and procedures for developing the test, criterion focuses on external measures of the same target construct, and construct focuses on the theory underlying the construct and includes relationships with other measures. In certain testing situations, one source of validity evidence may be more relevant than another. However, all three are often used together to argue that the evidence supporting a test is adequate. We will review each source of validity evidence in detail, and go over some practical examples of when one is more relevant than another. In this discussion, consider your own example, and examples of other tests youve encountered, and what type of validity evidence could be used to support their use. 9.3 Content validity According to Haynes, Richard, and Kubany (1995), content validity is the degree to which elements of an assessment instrument are relevant to and representative of the targeted construct for a particular assessment purpose. Note that this definition of content validity is very similar to our original definition of validity. The difference is that content validity focuses on elements of the construct and how well they are represented in our test. Thus, content validity assumes the target construct can be broken down into elements, and that we can obtain a representative sample of these elements. Having defined the purpose of our test and the construct we are measuring, there are three main steps to establishing content validity evidence: Define the content domain based on relevant standards, skills, tasks, behaviors, facets, factors, etc. that represent the construct. The idea here is that our construct can be represented in terms of specific identifiable dimensions or components, some of which may be more relevant to the construct than others. Use the defined content domain to create a blueprint or outline for our test. The blueprint organizes the test based on the relevant components of the content domain, and describes how each of these components will be represented within the test. Subject matter experts evaluate the extent to which our test blueprint adequately captures the content domain, and the extent to which our test items will adequately sample from the content domain. Here is an overview of how content validity could be established for the IGDI measures of early literacy. Again, the purpose of the test is to identify preschoolers in need of additional support in developing early literacy skills. 1. Define the content domain The early literacy content domain is broken down into a variety of content areas, including alphabet principles (e.g., knowledge of the names and sounds of letters), phonemic awareness (e.g., awareness of the sounds that make up words), and oral language (e.g., definitional vocabulary). The literature on early literacy has identified other important skills, but well focus here on these three. Note that the content domain for a construct should be established both by research and practice. 2. Outline the test Next, we map the portions of our test that will address each area of the content domain. The test outline can include information about the type of items used, the cognitive skills required, and the difficulty levels that are targeted, among other things. Review Chapter ?? for additional details on test outlines or blueprints. Table 9.1 contains an example of a test outline for the IGDI measures. The three content areas listed above are shown in the first column. These are then broken down further into cognitive processes or skills. Theory and practical constraints determine reasonable numbers and types of test items or tasks devoted to each cognitive process in the test itself. The final column shows the percentage of the total test that is devoted to each area. Table 9.1: Example Test Outline for a Measure of Early Literacy Content Area Cognitive process Items Weight Alphabet principles Letter naming 20 13% Sound identification 20 13% Phonological awareness Rhyming 15 10% Alliteration 15 10% Sound blending 10 7% Oral language Picture naming 30 20% Which one doesnt belong 20 13% Sentence completion 20 13% 3. Evaluate Validity evidence requires that the test outline be representative of the content domain and appropriate for the construct and test purpose. The appropriateness of an outline is typically evaluated by content experts. In the case of the IGDI measures, these experts could be researchers in the area of early literacy, and teachers who work directly with students from the target population. Licensure testing Here is an example of content validity from the area of licensure/certification testing. I have consulted with an organization that develops and administers tests of medical imaging, including knowledge assessments taken by candidates for certification in radiography. This area provides a unique example of content validity, because the test itself measures a construct that is directly tied to professional practice. If practicing radiographers utilize a certain procedure, that procedure, or the knowledge required to perform it, should be included in the test. The domain for a licensure/certification test such as this is defined using what is referred to as a job analysis or practice analysis (Raymond 2001). A job analysis is a research study, the central feature of which is a survey sent to practitioners that lists a wide range of procedures and skills potentially used in the field. Respondents indicate how often they perform each procedure or use each skill on the survey. Procedures and skills performed by a high percentage of professionals are then included in the test outline. As in the previous examples, the final step in establishing content validity is having a select group of experts review the procedures and skills and their distribution across the test, as organized in the test outline. Psychological measures Content validity is relevant in non-cognitive psychological testing as well. Suppose the purpose of a test is to measure client experience with panic attacks so as to determine the efficacy of treatment. The domain for this test could be defined using criteria listed in the DSM-V (www.dsm5.org), reports about panic attack frequency, and secondary effects of panic attacks. The test outline would organize the number and types of items written to address all relevant criteria from the DSM-V. Finally, experts who work directly in clinical settings would evaluate the test outline to determine its quality, and their evaluation would provide evidence supporting the content validity of the test for this purpose. Threats to content validity When considering the appropriateness of our test content, we must also be aware of how content validity evidence can be compromised. What does content invalidity look like? For example, if our panic attack scores were not valid for a particular use, how would this lack of validity manifest itself in the process of establishing content validity? Here are two main sources of content invalidity. First, if items reflecting domain elements that are important to the construct are omitted from our test outline, the construct will be underrepresented in the test. In our panic attack example, if the test does not include items addressing nausea or abdominal distress, other criteria, such as fear of dying, may have too much sway in determining an individuals score. Second, if unnecessary items measuring irrelevant or tangential material are included, the construct will be misrepresented in the test. For example, if items measuring depression are included in the scoring process, the score itself is less valid as a measure of the target construct. Together, these two threats to content validity lead to unsupported score inferences. Some worst-case-scenario consequences include misdiagnoses, failure to provide needed treatment, or the provision of treatment that is not needed. In licensure testing, the result can be the licensing of candidates who lack the knowledge, skills, and abilities required for safe and effective practice. 9.4 Criterion validity 9.4.1 Definition Criterion validity is the degree to which test scores correlate with, predict, or inform decisions regarding another measure or outcome. If you think of content validity as the extent to which a test correlates with or corresponds to the content domain, criterion validity is similar in that it is the extent to which a test correlates with or corresponds to another test. So, in content validity we compare our test to the content domain, and hope for a strong relationship, and in criterion validity we compare our test to a criterion variable, and again hope for a strong relationship. Validity by association The keyword in this definition of criterion validity is correlate, which is synonymous with relate or predict. The assumption here is that the construct we are hoping to measure with our test is known to be measured well by another test or observed variable. This other test or variable is often referred to as a gold standard, a label presumably given to it because it is based on strong validity evidence. So, in a way, criterion validity is a form of validity by association. If our test correlates with a known measure of the construct, we can be more confident that our test measures the same construct. The equation for a validity coefficient is the same as the equations for correlation that we encountered in previous chapters. Here we denote our test as \\(X\\) and the criterion variable as \\(Y\\). The validity coefficient is the correlation between the two, which can be obtained as the covariance divided by the product of the individual standard deviations. \\[\\begin{equation} \\rho_{XY} = \\frac{\\sigma_{XY}}{\\sigma_{X}\\sigma_{Y}} \\tag{9.1} \\end{equation}\\] Criterion validity is sometimes distinguished further as concurrent validity, where our test and the criterion are administered concurrently, or predictive validity, where our test is measured first and can then be used to predict the future criterion. The distinction is based on the intended use of scores from our test for predictive purposes. Criterion validity is limited because it does not actually require that our test be a reasonable measure of the construct, only that it relate strongly with another measure of the construct. Nunnally and Bernstein (1994) clarify this point with a hypothetical example: If it were found that accuracy in horseshoe pitching correlated highly with success in college, horseshoe pitching would be a valid measure for predicting success in college. The scenario is silly, but it highlights the fact that, on its own, criterion validity is insufficient. The take-home message is that you should never use or trust a criterion relationship as your sole source of validity evidence. There are two other challenges associated with criterion validity. First, finding a suitable criterion can be difficult, especially if your test targets a new or not well defined construct. Second, a correlation coefficient is attenuated, or reduced in strength, by any unreliability present in the two measures being correlated. So, if your test and the criterion test are unreliable, a low validity coefficient (the correlation between the two tests) may not necessarily represent a lack of relationship between the two tests. It may instead represent a lack of reliable information with which to estimate the criterion validity coefficient. Attenuation Heres a demonstration of how attenuation works, based on PISA. Suppose we find a gold standard criterion measure of reading ability, and administer it to students in the US who took the reading items in PISA09. First, we calculate a total score on the PISA reading items, then we compare it to some simulated test scores for our criterion test. Scores have been simulated to correlate at 0.80. # Get the vector of reading items names ritems &lt;- c(&quot;r414q02&quot;, &quot;r414q11&quot;, &quot;r414q06&quot;, &quot;r414q09&quot;, &quot;r452q03&quot;, &quot;r452q04&quot;, &quot;r452q06&quot;, &quot;r452q07&quot;, &quot;r458q01&quot;, &quot;r458q07&quot;, &quot;r458q04&quot;) rsitems &lt;- paste0(ritems, &quot;s&quot;) # Calculate total reading scores pisausa &lt;- PISA09[PISA09$cnt == &quot;USA&quot;, rsitems] rtotal &lt;- rowSums(pisausa, na.rm = TRUE) # Simulate a criterion - see Chapter 5 for another example # using rsim criterion &lt;- rsim(rho = .8, x = rtotal, meany = 24, sdy = 6) # Check the correlation cor(rtotal, criterion$y) #&gt; [1] 0.802 Suppose the internal consistency reliability for our criterion is 0.86. We know from Chapter ?? that internal consistency for the PISA09 reading items is about 0.77. With a simple formula, we can calculate what the validity coefficient should be for our two measures, if each measure were perfectly reliable. Here, we denote this disattenuated correlation as the correlation between true scores on \\(X\\) and \\(Y\\). \\[\\begin{equation} \\rho_{T_X T_Y} = \\frac{\\rho_{XY}}{\\sqrt{\\rho_{X}\\rho_{Y}}} \\tag{9.2} \\end{equation}\\] Correcting for attenuation due to measurement error produces a validity coefficient of 0.99. This is a noteworthy increase from the original correlation of 0.80. # Internal consistency for the PISA items epmr::coef_alpha(pisausa) #&gt; $alpha #&gt; [1] 0.774 #&gt; #&gt; $q #&gt; NULL #&gt; #&gt; $se #&gt; NULL #&gt; #&gt; $ci #&gt; NULL #&gt; #&gt; $sigma #&gt; r414q02s r414q11s r414q06s r414q09s r452q03s r452q04s r452q06s #&gt; r414q02s 0.2499 0.0464 0.0777 0.0560 0.0325 0.0492 0.0736 #&gt; r414q11s 0.0464 0.2454 0.0526 0.0359 0.0165 0.0287 0.0421 #&gt; r414q06s 0.0777 0.0526 0.2492 0.0712 0.0299 0.0696 0.0971 #&gt; r414q09s 0.0560 0.0359 0.0712 0.2112 0.0210 0.0447 0.0747 #&gt; r452q03s 0.0325 0.0165 0.0299 0.0210 0.1092 0.0257 0.0393 #&gt; r452q04s 0.0492 0.0287 0.0696 0.0447 0.0257 0.2371 0.0777 #&gt; r452q06s 0.0736 0.0421 0.0971 0.0747 0.0393 0.0777 0.2486 #&gt; r452q07s 0.0740 0.0489 0.0820 0.0506 0.0431 0.0648 0.0831 #&gt; r458q01s 0.0627 0.0356 0.0805 0.0524 0.0325 0.0554 0.0728 #&gt; r458q07s 0.0634 0.0395 0.0856 0.0686 0.0304 0.0628 0.0825 #&gt; r458q04s 0.0532 0.0396 0.0505 0.0347 0.0246 0.0497 0.0703 #&gt; r452q07s r458q01s r458q07s r458q04s #&gt; r414q02s 0.0740 0.0627 0.0634 0.0532 #&gt; r414q11s 0.0489 0.0356 0.0395 0.0396 #&gt; r414q06s 0.0820 0.0805 0.0856 0.0505 #&gt; r414q09s 0.0506 0.0524 0.0686 0.0347 #&gt; r452q03s 0.0431 0.0325 0.0304 0.0246 #&gt; r452q04s 0.0648 0.0554 0.0628 0.0497 #&gt; r452q06s 0.0831 0.0728 0.0825 0.0703 #&gt; r452q07s 0.2466 0.0649 0.0678 0.0508 #&gt; r458q01s 0.0649 0.2478 0.0765 0.0462 #&gt; r458q07s 0.0678 0.0765 0.2410 0.0512 #&gt; r458q04s 0.0508 0.0462 0.0512 0.2501 #&gt; #&gt; $n #&gt; [1] 1611 #&gt; #&gt; $ni #&gt; [1] 11 # Correction for attenuation cor(rtotal, criterion$y)/sqrt(.77 * .86) #&gt; [1] 0.985 In summary, the steps for establishing criterion validity evidence are relatively simple. After defining the purpose of the test, a suitable criterion is identified. The two tests are administered to the same sample of individuals from the target population, and a correlation is obtained. If reliability estimates are available, we can then estimate the disattenuated coefficient, as shown above. Note that a variety of other statistics are available for establishing the predictive power of a test \\(X\\) for a criterion variable \\(Y\\). Two popular examples are regression models, which provide more detailed information about the bivariate relationship between our test and criterion, and contingency tables, which describe predictions in terms of categorical or ordinal outcomes. In each case, criterion validity can be maximized by writing items for our test that are predictive of or correlate with the criterion. 9.4.2 Criterion examples A popular example of criterion validity is the GRE, which has come up numerous times in this book. The GRE is designed to predict performance in graduate school. Admissions programs use it as one indicator of how well you are likely to do as a graduate student. Given this purpose, what is a suitable criterion variable that the GRE should predict? And how strong of a correlation would you expect to see between the GRE and this graduate performance variable? The simplest criterion for establishing criterion-related validity evidence for the GRE would be some measure of performance or achievement in graduate school. First-year graduate GPA is a common choice. The GRE has been shown across numerous studies to correlate around 0.30 with first-year graduate GPA. A correlation of 0.30 is evidence of a small positive relationship, suggesting that most of the variability in GPA, our criterion, is not predicted by the GRE (91% to be precise). In other words, many students score high or low on the GRE and do not have a similarly high or low graduate GPA. Although this modest correlation may at first seem disappointing, a few different considerations suggest that it is actually pretty impressive. First, GPA is likely not a reliable measure of graduate performance. Its hardly a gold standard. Instead, its the best we have. Its one of the few quantitative measures available for all graduate students. The correlation of 0.30 is likely attenuated due to, at least, measurement error in the criterion. Second, there is likely some restriction of range happening in the relationship between GRE and GPA. People who score low on the GRE are less likely to get into graduate school, so their data are not represented. Restriction of range tends to reduce correlation coefficients. Third, what other measure of pre-graduate school performance correlates at 0.30 with graduate GPA? More importantly, what other measure of pre-graduate school performance that only takes a few hours to obtain correlates at 0.30 with graduate GPA? In conclusion, the GRE isnt perfect, but as far as standardized predictors go, its currently the best weve got. In practice, admissions programs need to make sure they dont rely too much on it in admissions decisions, as discussed in Chapter ??. Note that a substantial amount of research has been conducted documenting predictive validity evidence for the GRE. See Kuncel, Hezlett, and Ones (2001) for a meta-analysis of results from this literature. 9.5 Construct validity 9.5.1 Definition As noted above, validity focuses on the extent to which our construct is in fact what we think it is. If our construct is what we think it is, it should relate in known ways with other measures of the same or different constructs. On the other hand, if it is not what we think it is, relationships that should exist with other constructs will not be found. Construct validity is established when relationships between our test and other variables confirm what is predicted by theory. For example, theory might indicate that the personality traits of conscientiousness and neuroticism should be negatively related. If we develop a test of conscientiousness and then demonstrate that scores on our test correlate negatively with scores on a test of neuroticism, weve established construct validity evidence for our test. Furthermore, theory might indicate that conscientiousness contains three specific dimensions. Statistical analysis of the items within our test could show that the items tend to cluster, or perform similarly, in three specific groups. This too would establish construct validity evidence for our test. Chapter ?? presents EFA and CFA as tools for exploring and confirming the factor structure of a test. Results from these analysis, particularly CFA, are key to establishing construct validity evidence. 9.5.2 Examples The entire set of relationships between our construct and other available constructs is sometimes referred to as a nomological network. This network outlines what the construct is, based on what it relates positively with, and conversely what it is not, based on what it relates negatively with. For example, what variables would you expect to relate positively with depression? As a person gets more depressed, what else tends to increase? What variables would you not expect to correlate with depression? Finally, what variables would you expect to relate negatively with depression? Table 9.2 contains an example of a correlation matrix that describes a nomological network for a hypothetical new depression scale. The BDI would be considered a well known criterion measure of depression. The remaining labels in this table refer to other related or unrelated variables. Fake bad is a measure of a persons tendency to pretend to be bad or associate themselves with negative behaviors or characteristics. Positive correlations in this table represent what is referred to as convergence. Our hypothetical new scale converges with the BDI, a measure of anxiety, and a measure of faking bad. Negative correlations represent divergence. Our new scale diverges with measures of happiness and health. Both types of correlation should be predicted by a theory of depression. Table 9.2: Nomological Network for a Hypothetical Depression Inventory New Scale BDI Anxiety Happy Health Fake Bad New Scale 1.00 BDI 0.80 1.00 Anxiety 0.65 0.50 1.00 Happy -0.59 -0.61 -0.40 1.00 Health -0.35 -0.10 -0.35 0.32 1.00 Fake Bad 0.10 0.14 0.07 -0.05 0.07 1.00 9.6 Unified validity and threats In the early 1980s, the three types of validity were reconceptualized as a single construct validity (e.g., Messick 1980). This reconceptualization clarifies how content and criterion evidence do not, on their own, establish validity. Instead, both contribute to an overarching evaluation of construct validity. The literature has also clarified that validation is an ongoing process, where evidence supporting test use is accumulated over time from multiple sources. As a result, validity is a matter of degree instead of being evaluated as a simple and absolute yes or no. As should be clear, scores are valid measures of a construct when they accurately represent the construct. When they do not, they are not valid. Two types of threats to content validity were mentioned previously. These are content underrepresentation and content misrepresentation. These can both be extended to more broadly refer to construct underrepresentation and construct misrepresentation. In the first, we fail to include all aspects of the construct in our test. In the second, our test is impacted by variables or constructs other than our target construct, including systematic and random error. And in both, we introduce construct irrelevant variance into our scores. Construct underrepresentation and misrepresentation can both be identified using a test outline. If the content domain is missing an important aspect of the construct, or the test is missing an important aspect of the content domain, the outline should make it apparent. Subject matter experts provide an external evaluation of these issues. Unfortunately, the construct is often underrepresented or misrepresented by individual items, and item-level content information is not provided in the test outline. As a result, the test development process also involves item-level reviews by subject matter experts and others who provide input on potential bias and unreliability at the item level. Underrepresenting or misrepresenting the construct in a test can have a negative impact on testing outcomes, both at the item level and the test level. Item bias refers to differential performance for subgroups of individuals, where the performance difference is not related to true differences in ability or trait. An item may address content that is relevant to the content domain, but it may do so in a way that is less easily understood by one group than another. For example, in educational tests, questions often involve word problems that provide context to an application. This context may address material, for example, a vacation to the beach, that is more familiar to students from a particular region, for example, coastal areas. This item might be biased against students who less familiar with the context because they dont live near the beach. Given that we arent interested in measuring proximity to coastline, this constitutes test bias that reduces the validity of our test scores. Other specific results of invalid test scores include misplacement of students, misallocation of funding, and implementation of programs that should not be implemented. Can you think of anything else to add to the list? What are some practical consequences of using test scores that do not measure what they purport to measure? 9.7 Summary This chapter provides an overview of validity, with examples of content, criterion, and construct validity, and details on how these three sources of validity evidence come together to support the intended interpretations and uses of test scores. The validation process is an incremental one, where sound test development practices and strong empirical results accumulate to establish 9.7.1 Exercises Consider your own testing application and how you would define a content domain. What is this definition of the content domain based on? In education, for example, end-of-year testing, its typically based on a curriculum. In psychology, its typically based on research and practice. How would you confirm that this content domain is adequate or representative of the construct? And how could content validity be compromised for your test? Consider your own testing application and a potential criterion measure for it. How do you go about choosing the criterion? How would you confirm that a relationship exists between your test and the criterion? How could criterion validity be compromised in this case? Construct underrepresentation and misrepresentation are reviewed briefly for the hypothetical test of panic attacks. Explain how underrepresentation and misrepresentation could each impact content validity for the early literacy measures. Suppose a medical licensure test correlates at 0.50 with a criterion measure based on supervisor evaluations of practicing physicians who have passed the test. Interpret this correlation as a validity coefficient, discussing the issues presented in this chapter. Consider what threats might impact the validity of your own testing application. How could these threats be identified in practice? How could they be avoided through sound test development? "],["good-resources.html", "10 Good Resources", " 10 Good Resources https://psychnerdjae.github.io/into-the-tidyverse/ Automatic Grading with RMarkdown example Git/Github for virtual learning (from this tweet) Learn-Datascience-for-Free https://allisonhorst.shinyapps.io/dplyr-learnr/ "],["references.html", "References", " References AERA, APA, and NCME. 1999. Standards for educational and psychological testing. Washington DC: American Educational Research Association. Ebel, R. 1961. Must All Tests Be Valid? American Psychologist 16 (640647). Haynes, S. N., D. C. S. Richard, and E. S. Kubany. 1995. Content Validity in Psychological Assessment: A Functional Approach to Concepts and Methods. Psychological Assessment 7: 23847. Kane, M. T. 2013. Validating the Interpretations and Uses of Test Scores. Journal of Educational Measurement 50: 173. Kuncel, N. R., S. A. Hezlett, and D. S. Ones. 2001. A Comprehensive Meta-Analysis of the Predictive Validity of the Graduate Record Examinations: Implications for Graduate Student Selection and Performance. Psychological Bulletin 127: 16281. Messick, S. 1980. Test validity and the ethics of assessment. American Psychologist 35: 101227. Nunnally, J. C, and I. H. Bernstein. 1994. Psychometric Theory. New York, NY: McGraw-Hill. Raymond, M. 2001. Job Analysis and the Specification of Content for Licensure and Certification Examinations. Applied Measurement in Education 14: 369415. "]]
