[["index.html", "Psychological Testing Course Notes Welcome to PSY 362", " Psychological Testing Course Notes S. Mason Garrison 2021-03-01 Welcome to PSY 362 Welcome to class! This website is designed to accompany Mason Garrisons Psychological Testing (Psych Testing). Psych Testing is a undergraduate-level quantitative methods course at Wake Forest University. This website hosts the course notes. Im glad you found me and the notes! They only cover the R aspects of this class. Why are the notes over here? And not on canvas? Well, that is a good question! Frankly, because this is much much easier for me to create. Moreover, if Im teaching you about R, you might as well learn it using R. What does that mean? Well, these notes are written in R (and RMarkdown)! Indeed, this entire website is written in R! You can find the current version of the course syllabus here, along with all of the other syllabi for my classes. "],["attribution.html", "Attribution Major &amp; Additional Attributions", " Attribution This class leans heavily on other peoples materials and ideas. I have done my best to document the origin of the materials and ideas. In particular, I have noted those people whose work has been a major contribution as well as those who have additional contributions. You can see specific changes by examining the edit history on the git repo Major &amp; Additional Attributions Jenny Bryans (jennybryan.org) STAT 545. Anthony Albanos Course Notes on Introduction to Educational and Psychological Measurement Bill Revelles An introduction to psychometric theory with applications in R "],["license.html", "License", " License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by-sa/4.0/legalcode for the full legal text. You are free to: Sharecopy and redistribute the material in any medium or format Remixremix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: AttributionYou must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlikeIf you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictionsYou may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. "],["colophon.html", "Colophon", " Colophon This book was written in bookdown inside RStudio. The website is hosted with github, The complete source is available from GitHub. The book style was designed by Desir√©e De Leon. This version of the book was built with: #&gt; Finding R package dependencies ... Done! #&gt; setting value #&gt; version R version 4.0.3 (2020-10-10) #&gt; os Windows 10 x64 #&gt; system x86_64, mingw32 #&gt; ui RTerm #&gt; language (EN) #&gt; collate English_United States.1252 #&gt; ctype English_United States.1252 #&gt; tz America/New_York #&gt; date 2021-03-01 Along with these packages: "],["public-health-dashboards.html", "Public Health Dashboards Wake Forest Forsyth County North Carolina", " Public Health Dashboards Ok, so I know that this class is about testing, measurement, and R However, I figured that it might be helpful for us all to have some public health dashboards in one easy place. If you want, we can pretend that this section is course content related Wake Forest The embedded dashboards are maintained by Wake Forest University and are made using Microsoft Power BI. More info about the dashboard can be found here here Forsyth County The embedded maps are maintained by Forsyth Countys department of public health. More info here Vaccinations Case Counts North Carolina "],["meet-our-toolbox.html", "1 Meet our toolbox! 1.1 R 1.2 Getting Help with R", " 1 Meet our toolbox! You can follow along with the slides here if they do not appear below. 1.1 R R is both a programming language and software environment for statistical analysis. It differs from other software like SPSS in three key ways. First, R is free, no strings (or warranties) attached. Download it at cran.r-project.org. The popular editor RStudio is also available for free at rstudio.com. Second, R is open-source, with thousands of active contributors sharing add-on packages. See the full list at cran.r-project.org/web/packages (there are currently over 8,000 packages). Third, R is accessed primarily through code, rather than by pointing and clicking through drop-down menus and dialog windows. This third point is a road block to some, but it ends up being a strength in the long run. Note well be using two pieces of software. R is the software that runs all your analyses. RStudio is an Integrated Development Environment or IDE that simplifies your interaction with R. RStudio isnt essential, but it gives you nice features for saving your R code, organizing the output of your analyses, and managing your add-on packages. 1.1.1 Install R and RStudio At this point you should download and install R and RStudio using the links below. The internet abounds with helpful tips on installation and getting started. I made a video walking windows-folks through the process. library(vembedr) embed_url(&quot;https://www.youtube.com/watch?v=kVIZGCT5p9U&quot;) %&gt;% use_align(&quot;center&quot;) Install R, a free software environment for statistical computing and graphics from CRAN, the Comprehensive R Archive Network. I highly recommend you install a precompiled binary distribution for your operating system  use the links up at the top of the CRAN page linked above! Install RStudios IDE (stands for integrated development environment), a powerful user interface for R. Get the Open Source Edition of RStudio Desktop. You can run either the Preview version or the official releases available here. RStudio comes with a text editor, so there is no immediate need to install a separate stand-alone editor. If you have a pre-existing installation of R and/or RStudio, I highly recommend that you reinstall both and get as current as possible. It can be considerably harder to run old software than new. If you upgrade R, you will need to update any packages you have installed. The command below should get you started, though you may need to specify more arguments if, e.g., you have been using a non-default library for your packages. update.packages(ask = FALSE, checkBuilt = TRUE) Note: this code will only look for updates on CRAN. So if you use a package that lives only on GitHub or if you want a development version from GitHub, you will need to update manually, e.g. via devtools::install_github(). 1.1.2 Testing testing Do whatever is appropriate for your OS to launch RStudio. You should get a window similar to the screenshot you see here, but yours will be more boring because you havent written any code or made any figures yet! Put your cursor in the pane labeled Console, which is where you interact with the live R process. Create a simple object with code like x &lt;- 3 * 4 (followed by enter or return). Then inspect the x object by typing x followed by enter or return. You should see the value 12 print to screen. If yes, youve succeeded in installing R and RStudio. As noted above, R is accessed via code, primarily in the form of commands that youll type or paste in the R console. The R console is simply the window where your R code goes, and where output will appear. Note that RStudio will present you with multiple windows, one of which will be the R console. That said, when instructions here say to run code in R, this applies to R via RStudio as well. 1.1.3 Add-on packages R is an extensible system and many people share useful code they have developed as a package via CRAN and GitHub. To install a package from CRAN, for example the dplyr package for data manipulation, here is one way to do it in the R console (there are others). install.packages(&quot;dplyr&quot;, dependencies = TRUE) By including dependencies = TRUE, we are being explicit and extra-careful to install any additional packages the target package, dplyr in the example above, needs to have around. You could use the above method to install the following packages, all of which we will use: tidyr, package webpage ggplot2, package webpage psych, package webpage 1.2 Getting Help with R You can follow along with the slides here if they do not appear below. Help files in R are easily accessed by entering a function name preceded by a question mark, for example, ?c, ?mean, or ?devtools::install_github. Parentheses arent necessary. The question mark approach is a shortcut for the help() function, where, for example, ?mean is the same as help(\"mean\"). Either approach provides direct access to the R documentation for a function. The documentation for a function should at least give you a brief description of the function, definitions for the arguments that the function accepts, and examples of how the function can be used. At the console, you can also perform a search through all of the available R documentation using two question marks. For example, ??\"regression\" will search the R documentation for the term regression. This is a shortcut for the function help.search(). Finally, you can browse through the R documentation with help.start(). This will open a web browser with links to manuals and other resources. If youre unsatisfied with the documentation internal to R, an online search can be surprisingly effective for finding function names or instructions on running a certain procedure or analysis in R. 1.2.1 Further resources The above will get your basic setup ready but here are some links if you are interested in reading a bit further. How to Use RStudio RStudios leads for learning R R FAQ R Installation and Administration More about add-on packages in the R Installation and Administration Manual "],["r-basics.html", "2 R basics and workflows 2.1 Introducing Functions 2.2 Introduction to Objects 2.3 Workspace and working directory 2.4 RStudio projects 2.5 Saving Files and Protips", " 2 R basics and workflows Who is R? Why is R troubling PhD students?@AcademicChatter #AcademicTwitter&mdash; Dr. Marie Curie (@CurieDr) January 31, 2021 Well start our tour of R with a summary of how R code is used to interact with R via the console. In these notes, blocks of example R code are offset from the main text as shown below. Comments within code blocks start with a single hash #, the code itself has nothing consistent preceding it, and output from my R console is preceded by a double hash ##. You can copy and paste example code directly into your R console. Anything after the # will be ignored. # This is a comment within a block of R code. Comments # start with the hash sign and are ignored by R. The code # below will be interpreted by R once you paste or type it # into the console. x &lt;- c(4, 8, 15, 16, 23, 42) mean(x) # Only code after the hash is ignored #&gt; [1] 18 sd(x) #&gt; [1] 13.5 In the code above, were creating a short vector of scores in x and calculating its mean and standard deviation. Ok, if you havent opened up R/RStudio, do that now. Notice the default panes: Console (entire left) Environment/History (tabbed in upper right) Files/Plots/Packages/Help (tabbed in lower right) FYI: you can change the default location of the panes, among many other things: Customizing RStudio. Go into the Console, where we interact with the live R process. You should paste that example code in the console and verify that you get the same results. Note that code you enter at the console is preceded by the R prompt &gt;, whereas output printed in your console is not. 2.1 Introducing Functions x &lt;- c(4, 8, 15, 16, 23, 42) mean(x) #&gt; [1] 18 sd(x) #&gt; [1] 13.5 In the example code above (and in the previous section), the functions that get things done in R have names. To use them, we summon by name with parentheses enclosing any required information or instructions for how the functions should work. For example, we used the function c() to combine a set of numbers into a vector of scores. The information supplied to c() consisted of the scores themselves, separated by commas. mean() and sd() are functions for obtaining the mean and standard deviation of vectors of scores, like the ones in x. 2.2 Introduction to Objects x &lt;- c(4, 8, 15, 16, 23, 42) mean(x) #&gt; [1] 18 sd(x) #&gt; [1] 13.5 The second thing to notice in the example above is that data and results are saved to objects in R using the assignment operator &lt;-. We used the concatenate function to stick our numbers together in a set, c(4, 8, 15, 16, 23, 42), and then we assigned the result to have the name x. Objects created in this way can be accessed later on by their assigned name, for example, to find a mean or standard deviation. If we wanted to access it later, we could also save the mean of x to a new object. 2.2.0.1 Now you try! Make an assignment and then inspect the object you just created: x &lt;- 3 * 4 x #&gt; [1] 12 All R statements where you create objects  assignments  have this form: objectName &lt;- value and in my head I hear, e.g., x gets 12. Object names cannot start with a digit and cannot contain certain other characters such as a comma or a space. You will be wise to adopt a convention for demarcating words in names. i_use_snake_case other.people.use.periods evenOthersUseCamelCase Make another assignment: this_is_a_really_long_name &lt;- 2.5 To inspect this, try out RStudios completion facility: type the first few characters, press TAB, add characters until you disambiguate, then press return. Make another assignment: mason_rocks &lt;- 2 ^ 3 Lets try to inspect: masonrocks #&gt; Error in eval(expr, envir, enclos): object &#39;masonrocks&#39; not found masn_rocks #&gt; Error in eval(expr, envir, enclos): object &#39;masn_rocks&#39; not found Implicit contract with the computer / scripting language: Computer will do tedious computation for you. In return, you will be completely precise in your instructions. Typos matter. Case matters. mason_rocks #&gt; [1] 8 R has a mind-blowing collection of built-in functions that are accessed like so: functionName(arg1 = val1, arg2 = val2, and so on) Lets try using seq() which makes regular sequences of numbers and, while were at it, demo more helpful features of RStudio. Type se and hit TAB. A pop up shows you possible completions. Specify seq() by typing more to disambiguate or using the up/down arrows to select. Notice the floating tool-tip-type help that pops up, reminding you of a functions arguments. If you want even more help, press F1 as directed to get the full documentation in the help tab of the lower right pane. Now open the parentheses and notice the automatic addition of the closing parenthesis and the placement of cursor in the middle. Type the arguments 1, 10 and hit return. RStudio also exits the parenthetical expression for you. IDEs are great. seq(1, 10) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 The above also demonstrates something about how R resolves function arguments. You can always specify in name = value form. But if you do not, R attempts to resolve by position. So above, it is assumed that we want a sequence from = 1 that goes to = 10. Since we didnt specify step size, the default value of by in the function definition is used, which ends up being 1 in this case. For functions I call often, I might use this resolve by position for the first argument or maybe the first two. After that, I always use name = value. Make this assignment and notice similar help with quotation marks. yo &lt;- &quot;hello world&quot; If you just make an assignment, you dont get to see the value, so then youre tempted to immediately inspect. y &lt;- seq(1, 10) y #&gt; [1] 1 2 3 4 5 6 7 8 9 10 This common action can be shortened by surrounding the assignment with parentheses, which causes assignment and print to screen to happen. (y &lt;- seq(1, 10)) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 Not all functions have (or require) arguments: date() #&gt; [1] &quot;Mon Mar 01 14:36:10 2021&quot; Now look at your workspace  in the upper right pane. The workspace is where user-defined objects accumulate. You can also get a listing of these objects with commands: objects() #&gt; [1] &quot;check_quietly&quot; &quot;ds4p_urls&quot; #&gt; [3] &quot;install_quietly&quot; &quot;mason_rocks&quot; #&gt; [5] &quot;pretty_install&quot; &quot;sample_no_surprises&quot; #&gt; [7] &quot;shhh_check&quot; &quot;slide_url&quot; #&gt; [9] &quot;this_is_a_really_long_name&quot; &quot;x&quot; #&gt; [11] &quot;y&quot; &quot;yo&quot; ls() #&gt; [1] &quot;check_quietly&quot; &quot;ds4p_urls&quot; #&gt; [3] &quot;install_quietly&quot; &quot;mason_rocks&quot; #&gt; [5] &quot;pretty_install&quot; &quot;sample_no_surprises&quot; #&gt; [7] &quot;shhh_check&quot; &quot;slide_url&quot; #&gt; [9] &quot;this_is_a_really_long_name&quot; &quot;x&quot; #&gt; [11] &quot;y&quot; &quot;yo&quot; If you want to remove the object named y, you can do this: rm(y) To remove everything: rm(list = ls()) or click the broom in RStudios Environment pane. 2.3 Workspace and working directory One day you will need to quit R, go do something else and return to your analysis later. One day you will have multiple analyses going that use R and you want to keep them separate. One day you will need to bring data from the outside world into R and send numerical results and figures from R back out into the world. To handle these real life situations, you need to make two decisions: What about your analysis is real, i.e. will you save it as your lasting record of what happened? Where does your analysis live? 2.3.1 Workspace, .RData As a beginning R user, its OK to consider your workspace real. Very soon, I urge you to evolve to the next level, where you consider your saved R scripts as real. (In either case, of course the input data is very much real and requires preservation!) With the input data and the R code you used, you can reproduce everything. You can make your analysis fancier. You can get to the bottom of puzzling results and discover and fix bugs in your code. You can reuse the code to conduct similar analyses in new projects. You can remake a figure with different aspect ratio or save is as TIFF instead of PDF. You are ready to take questions. You are ready for the future. If you regard your workspace as real (saving and reloading all the time), if you need to redo analysis  youre going to either redo a lot of typing (making mistakes all the way) or will have to mine your R history for the commands you used. Rather than becoming an expert on managing the R history, a better use of your time and energy is to keep your good R code in a script for future reuse. Because it can be useful sometimes, note the commands youve recently run appear in the History pane. But you dont have to choose right now and the two strategies are not incompatible. Lets demo the save / reload the workspace approach. Upon quitting R, you have to decide if you want to save your workspace, for potential restoration the next time you launch R. Depending on your set up, R or your IDE, e.g. RStudio, will probably prompt you to make this decision. Quit R/RStudio, either from the menu, using a keyboard shortcut, or by typing q() in the Console. Youll get a prompt like this: Save workspace image to ~/.Rdata? Note where the workspace image is to be saved and then click Save. Using your favorite method, visit the directory where image was saved and verify there is a file named .RData. You will also see a file .Rhistory, holding the commands submitted in your recent session. Restart RStudio. In the Console you will see a line like this: [Workspace loaded from ~/.RData] indicating that your workspace has been restored. Look in the Workspace pane and youll see the same objects as before. In the History tab of the same pane, you should also see your command history. Youre back in business. This way of starting and stopping analytical work will not serve you well for long but its a start. 2.3.2 Working directory Any process running on your computer has a notion of its working directory. In R, this is where R will look, by default, for files you ask it to load. It also where, by default, any files you write to disk will go. Chances are your current working directory is the directory we inspected above, i.e. the one where RStudio wanted to save the workspace. You can explicitly check your working directory with: getwd() It is also displayed at the top of the RStudio console. As a beginning R user, its OK let your home directory or any other weird directory on your computer be Rs working directory. Very soon, I urge you to evolve to the next level, where you organize your projects into directories and, when working on project A, set Rs working directory to the associated directory. Although I do not recommend it, in case youre curious, you can set Rs working directory at the command line like so: setwd(&quot;~/myCoolProject&quot;) Although I do not recommend it, you can also use RStudios Files pane to navigate to a directory and then set it as working directory from the menu: Session &gt; Set Working Directory &gt; To Files Pane Location. (Youll see even more options there). Or within the Files pane, choose More and Set As Working Directory. But theres a better way. A way that also puts you on the path to managing your R work like an expert. 2.4 RStudio projects Keeping all the files associated with a project organized together  input data, R scripts, results, figures  is such a wise and common practice that RStudio has built-in support for this via its projects. Lets make one to use for the rest of this class. Do this: File &gt; New Project. The directory name you choose here will be the project name. Call it whatever you want (or follow me for convenience). I created a directory and, therefore RStudio project, called swc in my tmp directory, FYI. setwd(&quot;~/tmp/swc&quot;) Now check that the home directory for your project is the working directory of our current R process: getwd() I cant print my output here because this document itself does not reside in the RStudio Project we just created. Lets enter a few commands in the Console, as if we are just beginning a project: a &lt;- 2 b &lt;- -3 sig_sq &lt;- 0.5 x &lt;- runif(40) y &lt;- a + b * x + rnorm(40, sd = sqrt(sig_sq)) (avg_x &lt;- mean(x)) #&gt; [1] 0.547 write(avg_x, &quot;avg_x.txt&quot;) plot(x, y) abline(a, b, col = &quot;purple&quot;) dev.print(pdf, &quot;toy_line_plot.pdf&quot;) #&gt; png #&gt; 2 Lets say this is a good start of an analysis and your ready to start preserving the logic and code. Visit the History tab of the upper right pane. Select these commands. Click To Source. Now you have a new pane containing a nascent R script. Click on the floppy disk to save. Give it a name ending in .R or .r, I used toy-line.r and note that, by default, it will go in the directory associated with your project. Quit RStudio. Inspect the folder associated with your project if you wish. Maybe view the PDF in an external viewer. Restart RStudio. Notice that things, by default, restore to where we were earlier, e.g. objects in the workspace, the command history, which files are open for editing, where we are in the file system browser, the working directory for the R process, etc. These are all Good Things. Change some things about your code. Top priority would be to set a sample size n at the top, e.g. n &lt;- 40, and then replace all the hard-wired 40s with n. Change some other minor-but-detectable stuff, e.g. alter the sample size n, the slope of the line b,the color of the line  whatever. Practice the different ways to re-run the code: Walk through line by line by keyboard shortcut (Command+Enter) or mouse (click Run in the upper right corner of editor pane). Source the entire document  equivalent to entering source('toy-line.r') in the Console  by keyboard shortcut (Shift+Command+S) or mouse (click Source in the upper right corner of editor pane or select from the mini-menu accessible from the associated down triangle). Source with echo from the Source mini-menu. Visit your figure in an external viewer to verify that the PDF is changing as you expect. In your favorite OS-specific way, search your files for toy_line_plot.pdf and presumably you will find the PDF itself (no surprise) but also the script that created it (toy-line.r). This latter phenomenon is a huge win. One day you will want to remake a figure or just simply understand where it came from. If you rigorously save figures to file with R code and not ever ever ever the mouse or the clipboard, you will sing my praises one day. Trust me. 2.5 Saving Files and Protips It is traditional to save R scripts with a .R or .r suffix. Follow this convention unless you have some extraordinary reason not to. Comments start with one or more # symbols. Use them. RStudio helps you (de)comment selected lines with Ctrl+Shift+C (Windows and Linux) or Command+Shift+C (Mac). Clean out the workspace, i.e. pretend like youve just revisited this project after a long absence. The broom icon or rm(list = ls()). Good idea to do this, restart R (available from the Session menu), re-run your analysis to truly check that the code youre saving is complete and correct (or at least rule out obvious problems!). This workflow will serve you well in the future: Create an RStudio project for an analytical project Keep inputs there (well soon talk about importing) Keep scripts there; edit them, run them in bits or as a whole from there Keep outputs there (like the PDF written above) Avoid using the mouse for pieces of your analytical workflow, such as loading a dataset or saving a figure. Terribly important for reproducibility and for making it possible to retrospectively determine how a numerical table or PDF was actually produced (searching on local disk on filename, among .R files, will lead to the relevant script). Many long-time users never save the workspace, never save .RData files (Im one of them), never save or consult the history. Once/if you get to that point, there are options available in RStudio to disable the loading of .RData and permanently suppress the prompt on exit to save the workspace (go to Tools &gt; Options &gt; General). For the record, when loading data into R and/or writing outputs to file, you can always specify the absolute path and thereby insulate yourself from the current working directory. This method is rarely necessary when using RStudio projects. "],["refreshers-and-examples.html", "3 Refreshers and Examples 3.1 Some terms 3.2 Summary Statistics", " 3 Refreshers and Examples Youre now ready for a review of the introductory statistics that are prerequisite for the analyses that come later in this class. Statistics are important in measurement because they allow us to score and summarize the information collected with our tests and instruments. Theyre used to describe the reliability, validity, and predictive power of this information. Theyre also used to describe how well our test covers a domain of content or a network of constructs, including in relation to other content areas or constructs. We rely heavily on statistics for later modules 3.1 Some terms Well begin this review with some basic statistical terms. These ideas should be familiar to you from your research methods classes, but I figured that this information is useful in case youve gotten a little rusty. First, a variable is a set of values that can differ for different people. For example, we often measure variables such as age and gender. These words are italicized here to denote them as statistical variables, as opposed to words. The term variable is synonymous with quality, attribute, trait, or property. Constructs are also variables. Really, a variable is anything assigned to people that can potentially take on more than just a single constant value. As noted above, variables in R can be contained within simple vectors, for example, x, or they can be grouped together in a data.frame. Generic variables will be labeled in this book using capital letters, usually \\(X\\) and \\(Y\\). Here, \\(X\\) might represent a generic test score, for example, the total score across all the items in a test. It might also represent scores on a single item. Both are considered variables. The definition of a generic variable like \\(X\\) depends on the context in which it is defined. Indices can also be used to denote generic variables that are part of some sequence of variables. Most often this will be scores on items within a test, where, for example, \\(X_1\\) is the first item, \\(X_2\\) is the second, and \\(X_J\\) is the last, with \\(J\\) being the number of items in the test and \\(X_j\\) representing any given item. Subscripts can also be used to index individual people on a single variable. For example, test scores for a group of people could be denoted as \\(X_1\\), \\(X_2\\), \\(\\dots\\), \\(X_N\\), where \\(N\\) is the number of people and \\(X_i\\) represents the score for a generic person. Combining people and items, \\(X_{ij}\\) would be the score for person \\(i\\) on item \\(j\\). The number of people is denoted by \\(n\\) or sometimes \\(N\\). Typically, the lowercase \\(n\\) represents sample size and the uppercase \\(N\\) represents the population, however, the two are often used interchangeably. Greek and Arabic letters are used for other sample and population statistics. The sample mean is denoted by \\(m\\) and the population mean by \\(\\mu\\), the standard deviation is \\(s\\) or \\(\\sigma\\), variance is \\(s^2\\) or \\(\\sigma^2\\), and correlation is \\(r\\) or \\(\\rho\\). Note that the mean and standard deviation are sometimes abbreviated as \\(M\\) and \\(SD\\). Note also that distinctions between sample and population values often arent necessary, in which case the population terms are used. If a distinction is necessary, it will be identified. Finally, you may see named subscripts added to variable names and other terms, for example, \\(M_{control}\\) might denote the mean of a control group. These subscripts depend on the situation and must be interpreted in context. 3.2 Summary Statistics Descriptive and inferential are terms that refer to two general uses of statistics. These uses differ based on whether or not an inference is made from properties of a sample of data to parameters for an unknown population. Descriptive statistics, or descriptives, are used simply to explore and describe certain features of distributions. For example, the mean and variance are statistics identifying the center of and variability in a distribution. These and other statistics are used inferentially when an inference is made to a population Descriptives are not typically used to answer research questions or inform decision making. Instead, inferential statistics are more appropriate for these less exploratory and more confirmatory results. Inferential statistics involve an inference to a parameter or a population value. The quality of this inference is gauged using statistical tests that index the error associated with our estimates. In this review were focusing on descriptive statistics. Later well consider some inferential applications. The describe() function in the psych package returns basic descriptive statistics that are often useful for psychometrics, including the mean, median, standard deviation (sd), skewness (skew), kurtosis (kurt), minimum (min), and maximum (max), as well as some others Im forgetting. The describe function in the psych package is meant to produce the most frequently requested stats in psychometric and psychology studies, and to produce them in an easy to read data.frame. If a grouping variable is called for in formula mode, it will also call describeBy to the processing. The results from describe can be used in graphics functions (e.g., error.crosses). The range statistics (min, max, range) are most useful for data checking to detect coding errors, and should be found in early analyses of the data. Although describe will work on data frames as well as matrices, it is important to realize that for data frames, descriptive statistics will be reported only for those variables where this makes sense (i.e., not for alphanumeric data). If the check option is TRUE, variables that are categorical or logical are converted to numeric and then described. These variables are marked with an * in the row name. This is somewhat slower. Note that in the case of categories or factors, the numerical ordering is not necessarily the one expected. For instance, if education is coded high school, some college , finished college, then the default coding will lead to these as values of 2, 3, 1. Thus, statistics for those variables marked with * should be interpreted cautiously (if at all). "],["r-code-for-module-5.html", "4 R Code for Module 5 4.1 Simulate a constant true score 4.2 PISA total reading scores with simulated error and true scores based on CTT 4.3 Reliability and unreliability Illustrated", " 4 R Code for Module 5 4.1 Simulate a constant true score In this example, I simulate a constant true score, and randomly varying error scores from a normal population with mean 0 and SD 1 Note, set.seed() gives R a starting point for generating random numbers, so we can get the same results on different computers You should check the mean and SD of E and X. Creating a histogram of X might be interesting too library(tidyverse) set.seed(160416) myt &lt;- 20 mye &lt;- rnorm(1000, mean = 0, sd = 1) myx &lt;- myt + mye df=data.frame(myx,myt,mye) mean(myx); sd(myx) #&gt; [1] 20 #&gt; [1] 1.03 p &lt;- ggplot(df, aes(x=myx)) + geom_histogram() p #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 4.2 PISA total reading scores with simulated error and true scores based on CTT ## Libraries #install.packages(&quot;devtools&quot;) #devtools::install_github(&quot;talbano/epmr&quot;) library(epmr) #&gt; #&gt; Attaching package: &#39;epmr&#39; #&gt; The following object is masked from &#39;package:psych&#39;: #&gt; #&gt; skew library(ggplot2) ritems &lt;- c(&quot;r414q02&quot;, &quot;r414q11&quot;, &quot;r414q06&quot;, &quot;r414q09&quot;, &quot;r452q03&quot;, &quot;r452q04&quot;, &quot;r452q06&quot;, &quot;r452q07&quot;, &quot;r458q01&quot;, &quot;r458q07&quot;, &quot;r458q04&quot;) rsitems &lt;- paste0(ritems, &quot;s&quot;) BEL_PISA09=PISA09[PISA09$cnt == &quot;BEL&quot;, rsitems] xscores &lt;- rowSums(BEL_PISA09, na.rm = TRUE) Simulate error scores based on known SEM of 1.4, which well calculate later, then create true scores True scores are truncated to fall between 0 and 11 using setrange() escores &lt;- rnorm(length(xscores), 0, 1.4) tscores &lt;- setrange(xscores - escores, y = xscores) Combine in a data frame and create a scatterplot scores &lt;- data.frame(x1 = xscores, t = tscores, e = escores) ggplot(scores, aes(x1, t)) + geom_point(position = position_jitter(w = .3)) + geom_abline(col = &quot;blue&quot;) 4.3 Reliability and unreliability Illustrated Here we have simulated scores for a new form of the reading test called y. Note that rho is the made up reliability, which is set to 0.80, and x is the original reading total scores. Form y, which is slightly easier than x, has a mean of 6 and SD of 3. xysim &lt;- rsim(rho = .8, x = scores$x1, meany = 6, sdy = 3) scores$y &lt;- round(setrange(xysim$y, scores$x1)) ggplot(scores, aes(x1, y)) + geom_point(position = position_jitter(w = .3, h = .3)) + geom_abline(col = &quot;blue&quot;) Figure 4.1: PISA total reading scores and scores on a simulated second form of the reading test. "],["installing-epmr.html", "5 Installing epmr", " 5 Installing epmr A lot of the examples Im using in the class require the epmr package. In theory, it should be really easy to install, using the following code. # Will install devtools if not already installed if (!require(&quot;devtools&quot;)) install.packages(&quot;devtools&quot;) devtools::install_github(&quot;talbano/epmr&quot;) It will install devtools, if you dont have it already and then compile the current version of the package for you. However, if your computer doesnt already have a compiler, you might have some trouble. If you have trouble installing the epmr package, you are not alone. I, Prof. Mason, am working on resolving the issue. To install epmr package directly from github, you need devtools and a working development environment (a.k.a. an R friendly compiler). The specific compiler will depend on your operating system. Windows: Install Rtools. Mac: Install Xcode from the Mac App Store. Linux: Install a compiler and various development libraries (details vary across different flavors of Linux). Try installing the compiler that corresponds to your operating system. And then try installing directly from github again. # Will install devtools if not already installed if (!require(&quot;devtools&quot;)) install.packages(&quot;devtools&quot;) devtools::install_github(&quot;talbano/epmr&quot;) If the compilation doesnt work, dont despair! Ive compiled a version that should work. First download this file epmr_0.0.0.9000.tar.gz from github, and place it in your working directory. (Here is the direct download link:) Then, install that package directly, using this code. install.packages(&quot;epmr_0.0.0.9000.tar.gz&quot;, repos = NULL, type = &quot;source&quot;) If that doesnt work, please try using the Rstudio GUI, as illustrated below. Open the tools dropdown menu, and click Install Packages Then change the install from setting to Install from package archive Then select browse for file and locate the archive epmr_0.0.0.9000.tar.gz Then select that archive, and you should see something like the image below Press Install. This action should begin the installation process. If you still cannot get the package to install, please let me know as soon as possible. That way, I can help you during regular business hours. (In other words, dont wait until the last minute) "],["statistical-definition-of-reliability-.html", "6 Statistical definition of reliability. 6.1 Estimating reliability 6.2 Split-half method 6.3 Spearman Brown 6.4 SEM", " 6 Statistical definition of reliability. In CTT, reliability is defined as the proportion of variability in \\(X\\) that is due to variability in true scores \\(T\\): \\[\\begin{equation} r = \\frac{\\sigma^2_T}{\\sigma^2_X}. \\tag{6.1} \\end{equation}\\] 6.1 Estimating reliability One indirect estimate made possible by CTT is the correlation between scores on two forms of the same test: \\[\\begin{equation} r = \\rho_{X_1 X_2} = \\frac{\\sigma_{X_1 X_2}}{\\sigma_{X_1} \\sigma_{X_2}}. \\tag{6.2} \\end{equation}\\] 6.2 Split-half method The split-half method takes scores on a single test form, and separates them into scores on two halves of the test, which are treated as separate test forms. The correlation between these two halves then represents an indirect estimate of reliability, based on Equation (6.1). # Split half correlation, assuming we only had scores on # one test form # With an odd number of reading items, one half has 5 # items and the other has 6 library(epmr) xsplit1 &lt;- rowSums(PISA09[PISA09$cnt == &quot;BEL&quot;, rsitems[1:5]]) xsplit2 &lt;- rowSums(PISA09[PISA09$cnt == &quot;BEL&quot;, rsitems[6:11]]) cor(xsplit1, xsplit2, use = &quot;complete&quot;) #&gt; [1] 0.625 6.3 Spearman Brown The Spearman-Brown formula was originally used to correct for the reduction in reliability that occurred when correlating two test forms that were only half the length of the original test. In theory, reliability will increase as we add items to a test. Thus, Spearman-Brown is used to estimate, or predict, what the reliability would be if the half-length tests were made into full-length tests. # sb_r() in the epmr package uses the Spearman-Brown # formula to estimate how reliability would change when # test length changes by a factor k # If test length were doubled, k would be 2 sb_r(r = cor(xsplit1, xsplit2, use = &quot;complete&quot;), k = 2) #&gt; [1] 0.769 The Spearman-Brown reliability, \\(r_{new}\\), is estimated as a function of whats labeled here as the old reliability, \\(r_{old}\\), and the factor by which the length of \\(X\\) is predicted to change, \\(k\\): \\[\\begin{equation} r_{new} = \\frac{kr_{old}}{(k - 1)r_{old} + 1}. \\tag{6.3} \\end{equation}\\] Again, \\(k\\) is the factor by which the test length is increased or decreased. It is equal to the number of items in the new test divided by the number of items in the original test. Multiply \\(k\\) by the old reliability, and then divided the result by \\((k - 1)\\) times the old reliability, plus 1. The epmr package contains sb_r(), a simple function for estimating the Spearman-Brown reliability. 6.4 SEM Typically, were more interested in how the unreliability of a test can be expressed in terms of the available observed variability. Thus, we multiply the unreliable proportion of variance by the standard deviation of \\(X\\) to obtain the SEM: \\[\\begin{equation} SEM = \\sigma_X\\sqrt{1 - r}. \\tag{6.4} \\end{equation}\\] Confidence intervals for PISA09 can be estimated in the same way. First, we choose a measure of reliability, find the SD of observed scores, and obtain the corresponding SEM. Then, we can find the CI, which gives us the expected amount of uncertainty in our observed scores due to random measurement error. Here, were calculating SEM and the CI using alpha, but other reliability estimates would work as well. Figure 6.1 shows the 11 possible PISA09 reading scores in order, with error bars based on SEM for students in Belgium. # Get alpha and SEM for students in Belgium bela &lt;- coef_alpha(PISA09[PISA09$cnt == &quot;BEL&quot;, rsitems]) bela_alpha=bela$alpha # The sem function from epmr also overlaps with sem from # another package so we&#39;re spelling it out here in long # form belsem &lt;- epmr::sem(r = bela_alpha, sd = sd(scores$x1, na.rm = TRUE)) # Plot the 11 possible total scores against themselves # Error bars are shown for 1 SEM, giving a 68% confidence # interval and 2 SEM, giving the 95% confidence interval # x is converted to factor to show discrete values on the # x-axis beldat &lt;- data.frame(x = 1:11, sem = belsem) ggplot(beldat, aes(factor(x), x)) + geom_errorbar(aes(ymin = x - sem * 2, ymax = x + sem * 2), col = &quot;violet&quot;) + geom_errorbar(aes(ymin = x - sem, ymax = x + sem), col = &quot;yellow&quot;) + geom_point() Figure 6.1: The PISA09 reading scale shown with 68 and 95 percent confidence intervals around each point. Figure 6.1 helps us visualize the impact of unreliable measurement on score comparisons. For example, note that the top of the 95% confidence interval for \\(X\\) of 2 extends nearly to 5 points, and thus overlaps with the CI for adjacent scores 3 through 7. It isnt until \\(X\\) of 8 that the CI no longer overlap. With a CI of belsem 1.425, were 95% confident that students with observed scores differing at least by belsem * 4 5.7 have different true scores. Students with observed scores closer than belsem * 4 may actually have the same true scores. "],["interpreting-reliability-and-unreliability.html", "7 Interpreting reliability and unreliability", " 7 Interpreting reliability and unreliability There are no agreed-upon standards for interpreting reliability coefficients. Reliability is bound by 0 on the lower end and 1 at the upper end, because, by definition, the amount of true variability can never be less or more than the total available variability in \\(X\\). Higher reliability is clearly better, but cutoffs for acceptable levels of reliability vary for different fields, situations, and types of tests. The stakes of a test are an important consideration when interpreting reliability coefficients. The higher the stakes, the higher we expect reliability to be. Otherwise, cutoffs depend on the particular application. In general, reliabilities for educational and psychological tests can be interpreted using scales like the ones presented in Table 7.1. With medium-stakes tests, a reliability of 0.70 is sometimes considered minimally acceptable, 0.80 is decent, 0.90 is quite good, and anything above 0.90 is excellent. High stakes tests should have reliabilities at or above 0.90. Low stakes tests, which are often simpler and shorter than higher-stakes ones, often have reliabilities as low as 0.70. These are general guidelines, and interpretations can vary considerably by test. Remember that the cognitive measures in PISA would be considered low-stakes at the student level. A few additional considerations are necessary when interpreting coefficient alpha. First, alpha assumes that all items measure the same single construct. Items are also assumed to be equally related to this construct, that is, they are assumed to be parallel measures of the construct. When the items are not parallel measures of the construct, alpha is considered a lower-bound estimate of reliability, that is, the true reliability for the test is expected to be higher than indicated by alpha. Finally, alpha is not a measure of dimensionality. It is frequently claimed that a strong coefficient alpha supports the unidimensionality of a measure. However, alpha does not index dimensionality. It is impacted by the extent to which all of the test items measure a single construct, but it does not necessarily go up or down as a test becomes more or less unidimensional. Table 7.1: General Guidelines for Interpreting Reliability Coefficients Reliability High Stakes Interpretation Low Stakes Interpretation \\(\\geq 0.90\\) Excellent Excellent \\(0.80 \\leq r &lt; 0.90\\) Good Excellent \\(0.70 \\leq r &lt; 0.80\\) Acceptable Good \\(0.60 \\leq r &lt; 0.70\\) Borderline Acceptable \\(0.50 \\leq r &lt; 0.60\\) Low Borderline \\(0.20 \\leq r &lt; 0.50\\) Unacceptable Low \\(0.00 \\leq r &lt; 0.20\\) Unacceptable Unacceptable "],["reliability-study-designs.html", "8 Reliability study designs 8.1 Interrater reliability 8.2 Summary", " 8 Reliability study designs Now that weve established the more common estimates of reliability and unreliability, we can discuss the four main study designs that allow us to collect data for our estimates. These designs are referred to as internal consistency, equivalence, stability, and equivalence/stability designs. Each design produces a corresponding type of reliability that is expected to be impacted by different sources of measurement error. The four standard study designs vary in the number of test forms and the number of testing occasions involved in the study. Until now, weve been talking about using two test forms on two separate administrations. This study design is found in the lower right corner of Table 8.1, and it provides us with an estimate of equivalence (for two different forms of a test) and stability (across two different administrations of the test). This study design has the potential to capture the most sources of measurement error, and it can thus produce the lowest estimate of reliability, because of the two factors involved. The more time that passes between administrations, and as two test forms differ more in their content and other features, the more error we would expect to be introduced. On the other hand, as our two test forms are administered closer in time, we move from the lower right corner to the upper right corner of Table 8.1, and our estimate of reliability captures less of the measurement error introduced by the passage of time. Were left with an estimate of the equivalence between the two forms. As our test forms become more and more equivalent, we eventually end up with the same test form, and we move to the first column in Table 8.1, where one of two types of reliability is estimated. First, if we administer the same test twice with time passing between administrations, we have an estimate of the stability of our measurement over time. Given that the same test is given twice, any measurement error will be due to the passage of time, rather than differences between the test forms. Second, if we administer one test only once, we no longer have an estimate of stability, and we also no longer have an estimate of reliability that is based on correlation. Instead, we have an estimate of what is referred to as the internal consistency of the measurement. This is based on the relationships among the test items themselves, which we treat as miniature alternate forms of the test. The resulting reliability estimate is impacted by error that comes from the items themselves being unstable estimates of the construct of interest. Table 8.1: Four Main Reliability Study Designs 1 Form 2 Forms 1 Occasion Internal Consistency Equivalence 2 Occasions Stability Equivalence &amp; Stability Internal consistency reliability is estimated using either coefficient alpha or split-half reliability. All the remaining cells in Table 8.1 involve estimates of reliability that are based on correlation coefficients. Table 8.1 contains four commonly used reliability study designs. There are others, including designs based on more than two forms or more than two occasions, and designs involving scores from raters, discussed below. 8.1 Interrater reliability It was like listening to three cats getting strangled in an alley.  Simon Cowell, disparaging a singer on American Idol Interrater reliability can be considered a specific instance of reliability where inconsistencies are not attributed to differences in test forms, test items, or administration occasions, but to the scoring process itself, where humans, or in some cases computers, contribute as raters. Measurement with raters often involves some form of performance assessment, for example, a stage performance within a singing competition. Judgment and scoring of such a performance by raters introduces additional error into the measurement process. Interrater reliability allows us to examine the negative impact of this error on our results. Note that rater error is another factor or facet in the measurement process. Because it is another facet of measurement, raters can introduce error above and beyond error coming from sampling of items, differences in test forms, or the passage of time between administrations. This is made explicit within generalizability theory, discussed below. Some simpler methods for evaluating interrater reliability are introduced first. 8.1.1 Proportion agreement The proportion of agreement is the simplest measure of interrater reliability. It is calculated as the total number of times a set of ratings agree, divided by the total number of units of observation that are rated. The strengths of proportion agreement are that it is simple to calculate and it can be used with any type of discrete measurement scale. The major drawbacks are that it doesnt account for chance agreement between ratings, and it only utilizes the nominal information in a scale, that is, any ordering of values is ignored. To see the effects of chance, lets simulate scores from two judges where ratings are completely random, as if scores of 0 and 1 are given according to the flip of a coin. Suppose 0 is tails and 1 is heads. In this case, we would expect two raters to agree a certain proportion of the time by chance alone. The table() function creates a cross-tabulation of frequencies, also known as a crosstab. Frequencies for agreement are found in the diagonal cells, from upper left to lower right, and frequencies for disagreement are found everywhere else. # Simulate random coin flips for two raters # runif() generates random numbers from a uniform # distribution flip1 &lt;- round(runif(30)) flip2 &lt;- round(runif(30)) table(flip1, flip2) #&gt; flip2 #&gt; flip1 0 1 #&gt; 0 5 8 #&gt; 1 9 8 Lets find the proportion agreement for the simulated coin flip data. The question were answering is, how often did the coin flips have the same value, whether 0 or 1, for both raters across the 30 tosses? The crosstab shows this agreement in the first row and first column, with raters both flipping tails 5 times, and in the second row and second column, with raters both flipping heads 8 times. We can add these up to get 13, and divide by \\(n = 30\\) to get the percentage agreement. Data for the next few examples were simulated to represent scores given by two raters with a certain correlation, that is, a certain reliability. Thus, agreement here isnt simply by chance. In the population, scores from these raters correlated at 0.90. The score scale ranged from 0 to 6 points, with means set to 4 and 3 points for the raters 1 and 2, and SD of 1.5 for both. Well refer to these as essay scores, much like the essay scores on the analytical writing section of the GRE. Scores were also dichotomized around a hypothetical cut score of 3, resulting in either a Fail or Pass. # Simulate essay scores from two raters with a population # correlation of 0.90, and slightly different mean scores, # with score range 0 to 6 # Note the capital T is an abbreviation for TRUE essays &lt;- rsim(100, rho = .9, meanx = 4, meany = 3, sdx = 1.5, sdy = 1.5, to.data.frame = T) colnames(essays) &lt;- c(&quot;r1&quot;, &quot;r2&quot;) essays &lt;- round(setrange(essays, to = c(0, 6))) # Use a cut off of greater than or equal to 3 to determine # pass versus fail scores # ifelse() takes a vector of TRUEs and FALSEs as its first # argument, and returns here &quot;Pass&quot; for TRUE and &quot;Fail&quot; # for FALSE essays$f1 &lt;- factor(ifelse(essays$r1 &gt;= 3, &quot;Pass&quot;, &quot;Fail&quot;)) essays$f2 &lt;- factor(ifelse(essays$r2 &gt;= 3, &quot;Pass&quot;, &quot;Fail&quot;)) table(essays$f1, essays$f2) #&gt; #&gt; Fail Pass #&gt; Fail 19 0 #&gt; Pass 27 54 The upper left cell in the table() output above shows that for 19 individuals, the two raters both gave Fail. In the lower right cell, the two raters both gave Pass 54 times. Together, these two totals represent the agreement in ratings, 73 . The other cells in the table contain disagreements, where one rater said Pass but the other said Fail. Disagreements happened a total of 27 times. Based on these totals, what is the proportion agreement in the pass/fail ratings? Table 8.2 shows the full crosstab of raw scores from each rater, with scores from rater 1 (essays$r1) in rows and rater 2 (essays$r2) in columns. The bunching of scores around the diagonal from upper left to lower right shows the tendency for agreement in scores. Table 8.2: Crosstab of Scores From Rater 1 in Rows and Rater 2 in Columns 0 1 2 3 4 5 6 0 1 1 0 0 0 0 0 1 1 2 0 0 0 0 0 2 5 8 1 0 0 0 0 3 0 6 11 2 1 0 0 4 0 0 9 9 10 0 0 5 0 0 1 4 6 3 1 6 0 0 0 2 3 3 10 Proportion agreement for the full rating scale, as shown in Table 8.2, can be calculated by summing the agreement frequencies within the diagonal elements of the table, and dividing by the total number of people. # Pull the diagonal elements out of the crosstab with # diag(), sum them, and divide by the number of people sum(diag(table(essays$r1, essays$r2))) / nrow(essays) #&gt; [1] 0.29 Finally, lets consider the impact of chance agreement between one of the hypothetical human raters and a monkey who randomly applies ratings, regardless of the performance that is demonstrated, as with a coin toss. # Randomly sample from the vector c(&quot;Pass&quot;, &quot;Fail&quot;), # nrow(essays) times, with replacement # Without replacement, we&#39;d only have 2 values to sample # from monkey &lt;- sample(c(&quot;Pass&quot;, &quot;Fail&quot;), nrow(essays), replace = TRUE) table(essays$f1, monkey) #&gt; monkey #&gt; Fail Pass #&gt; Fail 10 9 #&gt; Pass 38 43 The results show that the hypothetical rater agrees with the monkey 53 percent of the time. Because we know that the monkeys ratings were completely random, we know that this proportion agreement is due entirely to chance. 8.1.2 Kappa agreement Proportion agreement is useful, but because it does not account for chance agreement, it should not be used as the only measure of interrater consistency. Kappa agreement is simply an adjusted form of proportion agreement that takes chance agreement into account. Equation (8.1) contains the formula for calculating kappa for two raters. \\[\\begin{equation} \\kappa = \\frac{P_o - P_c}{1 - P_c} \\tag{8.1} \\end{equation}\\] To obtain kappa, we first calculate the proportion of agreement, \\(P_o\\), as we did with the proportion agreement. This is calculated as the total for agreement divided by the total number of people being rated. Next we calculate the chance agreement, \\(P_c\\), which involves multiplying the row and column proportions (row and column totals divided by the total total) from the crosstab and then summing the result, as shown in Equation (8.2). \\[\\begin{equation} P_c = P_{first-row}P_{first-col} + P_{next-row}P_{next-col} + \\dots + P_{last-row}P_{last-col} \\tag{8.2} \\end{equation}\\] You do not need to commit Equations (8.1) and (8.2) to memory. Instead, theyre included here to help you understand that kappa involves removing chance agreement from the observed agreement, and then dividing this observed non-chance agreement by the total possible non-chance agreement, that is, \\(1 - P_c\\). The denominator for the kappa equation contains the maximum possible agreement beyond chance, and the numerator contains the actual observed agreement beyond chance. So, the maximum possible kappa is 1.0. In theory, we shouldnt ever observe less agreement than than expected by chance, which means that kappa should never be negative. Technically it is possible to have kappa below 0. When kappa is below 0, it indicates that our observed agreement is below what wed expect due to chance. Kappa should also be no larger than proportion agreement. In the example data, the proportion agreement decreased from 0.29 to 0.159 for kappa. A weighted version of the kappa index is also available. Weighted kappa let us reduce the negative impact of partial disagreements relative to more extreme disagreements in scores, by taking into account the ordinal nature of a score scale. For example, in Table 8.2, notice that only the diagonal elements of the crosstab measure perfect agreement in scores, and all other elements measure disagreements, even the ones that are close together like 2 and 3. With weighted kappa, we can give less weight to these smaller disagreements and more weight to larger disagreements such as scores of 0 and 6 in the lower left and upper right of the table. This weighting ends up giving us a higher agreement estimate. Here, we use the function astudy() from epmr to calculate proportion agreement, kappa, and weighted kappa indices. Weighted kappa gives us the highest estimate of agreement. Refer to the documentation for astudy() to see how the weights are calculated. # Use the astudy() function from epmr to measure agreement astudy(essays[, 1:2]) #&gt; agree kappa wkappa #&gt; 0.290 0.159 0.479 8.1.3 Pearson correlation The Pearson correlation coefficient, introduced above for CTT reliability, improves upon agreement indices by accounting for the ordinal nature of ratings without the need for explicit weighting. The correlation tells us how consistent raters are in their rank orderings of individuals. Rank orderings that are closer to being in agreement are automatically given more weight when determining the overall consistency of scores. The main limitation of the correlation coefficient is that it ignores systematic differences in ratings when focusing on their rank orders. This limitation has to do with the fact that correlations are oblivious to linear transformations of score scales. We can modify the mean or standard deviation of one or both variables being correlated and get the same result. So, if two raters provide consistently different ratings, for example, if one rater is more forgiving overall, the correlation coefficient can still be high as long as the rank ordering of individuals does not change. This limitation is evident in our simulated essay scores, where rater 2 gave lower scores on average than rater 1. If we subtract 1 point from every score for rater 2, the scores across raters will be more similar, as shown in Figure 8.1, but we still get the same interrater reliability. # Certain changes in descriptive statistics, like adding # constants won&#39;t impact correlations cor(essays$r1, essays$r2) #&gt; [1] 0.854 dstudy(essays[, 1:2]) #&gt; #&gt; Descriptive Study #&gt; #&gt; mean median sd skew kurt min max n na #&gt; r1 3.86 4 1.49 -0.270 2.48 0 6 100 0 #&gt; r2 2.88 3 1.72 0.242 2.15 0 6 100 0 cor(essays$r1, essays$r2 + 1) #&gt; [1] 0.854 A systematic difference in scores can be visualized by a consistent vertical or horizontal shift in the points within a scatter plot. Figure 8.1 shows that as scores are shifted higher for rater 2, they are more consistent with rater 1 in an absolute sense, despite the fact that the underlying linear relationship remains unchanged. # Comparing scatter plots ggplot(essays, aes(r1, r2)) + geom_point(position = position_jitter(w = .1, h = .1)) + geom_abline(col = &quot;blue&quot;) ggplot(essays, aes(r1, r2 + 1)) + geom_point(position = position_jitter(w = .1, h = .1)) + geom_abline(col = &quot;blue&quot;) Figure 8.1: Scatter plots of simulated essay scores with a systematic difference around 0.5 points. Is it a problem that the correlation ignores systematic score differences? Can you think of any real-life situations where it wouldnt be cause for concern? A simple example is when awarding scholarships or giving other types of awards or recognition. In these cases consistent rank ordering is key and systematic differences are less important because the purpose of the ranking is to identify the top candidate. There is no absolute scale on which subjects are rated. Instead, they are rated in comparison to one another. As a result, a systematic difference in ratings would technically not matter. 8.2 Summary This chapter provided an overview of reliability within the frameworks of CTT, for items and test forms, for reliability study designs with multiple facets. After a general definition of reliability in terms of consistency in scores, the CTT model was introduced, and two commonly used indices of CTT reliability were discussed: correlation and coefficient alpha. Reliability was then presented as it relates to consistency of scores from raters. Inter-rater agreement indices were compared, along with the correlation coefficient. "],["good-resources.html", "9 Good Resources", " 9 Good Resources https://psychnerdjae.github.io/into-the-tidyverse/ Automatic Grading with RMarkdown example Git/Github for virtual learning (from this tweet) Learn-Datascience-for-Free https://allisonhorst.shinyapps.io/dplyr-learnr/ "],["references.html", "References", " References "]]
